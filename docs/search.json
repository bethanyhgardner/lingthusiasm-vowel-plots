[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lingthusiasm Vowel Plots",
    "section": "",
    "text": "Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "2_annotate_audio.html",
    "href": "2_annotate_audio.html",
    "title": "Part 2: Annotating the Audio",
    "section": "",
    "text": "2.1 Setup\n\n\"\"\"Part 2 of Lingthusiasm Vowel Plots: Trimming Audio and Getting Vowel Formants.\"\"\"\n\n1import glob\nimport os\n2import pandas as pd\n3from pytube import Playlist, YouTube\n4from pydub import AudioSegment\n5import parselmouth\n\n\n1\n\nFile utilities.\n\n2\n\nDataframes.\n\n3\n\nGetting captions and audio data from YouTube.\n\n4\n\nWorking with audio files.\n\n5\n\nInterface with Praat.\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "1_find_words.html",
    "href": "1_find_words.html",
    "title": "Part 1: Finding Vowels to Plot",
    "section": "",
    "text": "There are going to be two sources of data, one more naturalistic and one more controlled:\n\nVowels pulled from the Lingthusiasm episode recordings\nVowels from Gretchen & Lauren recording the Wells lexical set\n\nThe first steps are to find the words for #1, and we’ll come back to #2 later.\n\n1.1 Setup\n\n\"\"\"Part 1 of Lingthusiasm Vowel Plots: Finding Vowels to Annotate.\"\"\"\n\n1import re\n2import pandas as pd\n3import requests\nfrom bs4 import BeautifulSoup\n4from thefuzz import process\n5from pytube import Playlist, YouTube\n\n\n1\n\nRegex functions.\n\n2\n\nDataframe functions.\n\n3\n\nScraping data from webpages.\n\n4\n\nFuzzy string matching.\n\n5\n\nGetting captions and audio from YouTube.\n\n\n\n\n(Note: All of this could also be done in R, but I’ve done it in Python here primarily because there are Python packages that can get data from YouTube without having to set up anything on the YouTube API side. Here, all you need to do is install the package.)\n\n\n1.2 Get Transcript Text & Speakers\n\n1.2.1 List Episodes\nThe Lingthusiasm website has a page listing all of the available transcripts. Step 1 is to load that page and get the list of URLs to the transcripts. (They have similar but not identical structures, so it’s easiest to read the list from the website instead of trying to construct them here.)\nThis function uses the BeautifulSoup package to return an HTML object and a text string for a URL:\n\ndef get_html_text(url):\n    \"\"\"Use BeautifulSoup to get the webpage text from the URL.\"\"\"\n\n1    resp = requests.get(url, timeout=1000)\n2    html = BeautifulSoup(resp.text, 'html.parser')\n\n3    return {'html': html, 'text': html.get_text()}\n\n\n1\n\nConnect to webpage.\n\n2\n\nLoad the HTML data from the webpage.\n\n3\n\nReturn the HTML data object and the text from the HTML data object.\n\n\n\n\nThis function uses BeautifulSoup to filter just the transcript URLs from the HTML data:\n\ndef get_transcript_links(url):\n    \"\"\"Get URLs to episode transcripts from HTML page.\"\"\"\n\n1    html = get_html_text(url)['html']\n2    url_objs = html.find_all('a')\n3    urls = [l.get('href') for l in url_objs]\n4    urls = pd.Series(data=urls, name='URL', dtype='string')\n5    urls = urls[urls.str.contains('transcript')]\n6    urls = urls[::-1]\n\n7    return urls.reset_index(drop=True)\n\n\n1\n\nGet HTML data from webpage using function defined above.\n\n2\n\nFilter using the a tag to get all of the link items.\n\n3\n\nGet the href item from each a tag item, which is the text of the URL.\n\n4\n\nConvert from list to pandas series.\n\n5\n\nFilter to only include URLs including the word transcript.\n\n6\n\nSort to have earliest episodes first.\n\n7\n\nReturn with index (row numbers) reset.\n\n\n\n\nThere are 84 episodes available (as of early October 2023):\n\n1transcript_urls = get_transcript_links('https://lingthusiasm.com/transcripts')\n\n2transcript_urls.head().to_frame().style\n\n\n1\n\nGet the URLs for the episode transcripts from the table of contents page, using the functions defined above.\n\n2\n\nTake the first 10 rows of the resulting list and print it as a nice table.\n\n\n\n\n\n\n\n\n\n \nURL\n\n\n\n\n0\nhttps://lingthusiasm.com/post/155357756341/transcript-lingthusiasm-episode-1-speaking-a\n\n\n1\nhttps://lingthusiasm.com/post/156181768226/transcript-lingthusiasm-episode-2-pronouns\n\n\n2\nhttps://lingthusiasm.com/post/157167562811/transcript-lingthusiasm-episode-3-arrival-of-the\n\n\n3\nhttps://lingthusiasm.com/post/157268108811/transcript-lingthusiasm-episode-4-inside-the-word\n\n\n4\nhttps://lingthusiasm.com/post/158014366301/transcript-lingthusiasm-episode-5-colour-words\n\n\n\n\n\nOnly keep episodes 1-84, for consistency replicating this code later:\n\ntranscript_urls = transcript_urls[:84]\n\n\n\n1.2.2 Scrape Transcript Text\nNow we can download transcripts, which are split into turns and labelled by speaker.\nSplitting out some sub-tasks into separate methods, this function gets the episode number (following episode-) as an integer from the URL:\n\ndef transcript_number_from_url(url):\n    \"\"\"Find transcript number in URL.\"\"\"\n\n1    index_episode = url.find('episode-')\n2    cur_number = url[index_episode + 8:]\n3    index_end = cur_number.find('-')\n    if index_end != -1:\n        cur_number = cur_number[:index_end]\n\n4    return int(cur_number)\n\n\n1\n\nFind location of episode-, since episode number is immediately following it.\n\n2\n\nSubset URL starting 8 characters after start of episode- string, which is immediately after it.\n\n3\n\nMost of the transcript URLs have more text after the number. If so, trim to just keep the number.\n\n4\n\nReturn episode number converted from string to integer.\n\n\n\n\nThe text returned from the transcript pages has information at the top and bottom that we don’t need. This is a transcript for marks the start of the transcript, and This work is licensed under marks the end. This function subsets the transcript dataframe rows to only include that section:\n\ndef trim_transcript(df):\n    \"\"\"Find start and end of transcript text.\"\"\"\n\n    start_index = df.find('This is a transcript for')\n    end_index = df.find('This work is licensed under')\n\n    return df[start_index:end_index]\n\nThis function cleans up the text column so it plays a bit nicer with Excel CSV export:\n\ndef clean_text(l):\n    \"\"\"Clean text column so it opens correctly as Excel CSV.\"\"\"\n\n1    l = l.strip()\n2    l = l.replace('\\u00A0', ' ').replace('–', '--').replace('…', '...')\n3    l = re.sub('“|”|‘|’', \"'\", l)\n\n4    return ' '.join(l.split())\n\n\n1\n\nRemove leading and trailing whitespaces.\n\n2\n\nReplace non-breaking spaces, en dashes, and ellipses.\n\n3\n\nReplace slanted quotes.\n\n4\n\nRemove double spaces between words.\n\n\n\n\nAfter a bit of trial and error, it’s easiest to split on the speaker names, since the paragraph formatting isn’t identical across all the pages:\n\nspeaker_names = [\n    'Gretchen', 'Lauren', 'Ake', 'Bona', 'Ev', 'Fei Ting', 'Gabrielle',\n    'Jade', 'Hannah', 'Hilaria', 'Janelle', 'Kat', 'Kirby', 'Lina', 'Nicole',\n    'Pedro', 'Randall', 'Shivonne', 'Suzy'\n]\n1speaker_regex = '(' + ':)|('.join(speaker_names) + ':)'\n\n\n1\n\nRegex looks like (Name1:)|(Name2:) etc. Surrounding names in parentheses makes it a capture group, so the names are included in the list of split items instead of dropped.\n\n\n\n\nThis function puts it all together to read the transcripts into one dataframe:\n\ndef get_transcripts(urls):\n    \"\"\"Get transcript text from URLs and format into dataframe.\"\"\"\n\n1    df = []\n    for l in urls:\n2        cur_text = get_html_text(l)['text']\n3        cur_text = trim_transcript(cur_text)\n4        cur_lines = re.split(speaker_regex, cur_text)\n5        cur_lines = [l for l in cur_lines if l is not None]\n6        cur_lines = cur_lines[1:]\n7        speakers = cur_lines[::2]\n        turns = cur_lines[1::2]\n8        cur_df = pd.DataFrame({\n            'Episode': transcript_number_from_url(l),\n            'Speaker': speakers,\n            'Text': [clean_text(line) for line in turns],\n        })\n        cur_df['Speaker'] = cur_df['Speaker'].str.removesuffix(':')\n9        cur_df['Turn'] = cur_df.index + 1\n10        cur_df = cur_df.set_index(['Episode', 'Turn'], drop=True)\n11        df.append(cur_df)\n\n12    df = pd.concat(df)\n13    df['Speaker'] = df['Speaker'].astype('category')\n    df['Text'] = df['Text'].astype('string')\n\n    return df\n\n\n1\n\nMake list to store results.\n\n2\n\nRead text string (vs HTML object) from URL, using function defined above.\n\n3\n\nTrim to only include transcript section, using function defined above.\n\n4\n\nSplit string into list with one item for each speaker turn, using regex defined above.\n\n5\n\nDrop items in list that are None (which are included because of capture group syntax).\n\n6\n\nDrop the initial “This is a transcript for…” line.\n\n7\n\nNow all the odd items are speaker labels and even items are turn text. [::2] is every 2nd item starting at 0, and [1::2] is every 2nd item starting at 1.\n\n8\n\nClean up strings and put everything into a dataframe.\n\n9\n\nMake column for turn number, which is index + 1.\n\n10\n\nSet Episode and Turn columns as indices.\n\n11\n\nAdd to list of parsed episodes.\n\n12\n\nCombine list of dataframes into one dataframe. This works because the Episode + Turn index combination is unique.\n\n13\n\nSet datatypes explicitly, to avoid warnings later.\n\n\n\n\nThis takes a minute or so to run:\n\n1transcripts = get_transcripts(transcript_urls)\n\n2pd.concat([transcripts.head(), transcripts.tail()]).style\n\n\n1\n\nRun get_transcripts() on the lists of links to each transcript, defined above.\n\n2\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \n \nSpeaker\nText\n\n\nEpisode\nTurn\n \n \n\n\n\n\n1\n1\nLauren\nWelcome to Lingthusiasm! A podcast that's enthusiastic about linguistics. I'm Lauren Gawne.\n\n\n2\nGretchen\nAnd I'm Gretchen McCulloch. And today we're going to be talking about universal language and why it doesn't work. But first a little bit about what Lingthusiasm is.\n\n\n3\nLauren\nSo I guess the important thing is to unpack 'a podcast that's enthusiastic about linguistics'.\n\n\n4\nGretchen\nYeah! We chose our tagline because we're here to explore interesting things that language has to offer and especially what looking at language from a linguistics perspective can tell us about how language works.\n\n\n5\nLauren\nBoth of us have blogs where we do a fair amount of that, but that's a fairly solitary enterprise and we wanted to take the opportunity to have more of a conversation -- something that's a little less disembodied\n\n\n84\n184\nLauren\nAh, well, that's okay, because now we're at the end of this episode, I'm pointing to everyone. [Music]\n\n\n185\nLauren\nFor more Lingthusiasm and links to all the things pointed to in this episode, go to lingthusiasm.com. You can listen to us on Apple Podcasts, Google Podcasts, Spotify, SoundCloud, YouTube, or wherever else you get your podcasts. You can follow @lingthusiasm on Twitter, Facebook, Instagram, and Tumblr. You can get 'Etymology isn't Destiny' on t-shirts and tote bags and lots of other items, and aesthetic IPA posters, and other Lingthusiasm merch at lingthusiasm.com/merch. I tweet and blog as Superlinguo.\n\n\n186\nGretchen\nI can be found as @GretchenAMcC on Twitter, my blog is AllThingsLinguistic.com, and my book about internet language is called Because Internet. Lingthusiasm is able to keep existing thanks to the support of our patrons. If you wanna get an extra Lingthusiasm episode to listen to every month, our entire archive of bonus episodes to listen to right now, or if you just wanna help keep the show running ad-free, go to patreon.com/lingthusiasm or follow the links from our website. Patrons can also get access to our Discord chatroom to talk with other linguistics fans and be the first to find out about new merch and other announcements. Recent bonus topics include interviews with Sarah Dopierala and Martha Tsutsui-Billins about their own linguistic research and their work on Lingthusiasm and a very special Lingthusiasmr episode where we read The Harvard Sentences to you [ASMR voice] in a calm, soothing voice. [Regular voice] Can't afford to pledge? That's okay, too. We also really appreciate it if you can recommend Lingthusiasm to anyone in your life who's curious about language.\n\n\n187\nLauren\nLingthusiasm is created and produced by Gretchen McCulloch and Lauren Gawne. Our Senior Producer is Claire Gawne, our Editorial Producer is Sarah Dopierala, our Production Assistant is Martha Tsutsui-Billins, and our Editorial Assistant is Jon Kruk. Our music is 'Ancient City' by The Triangles.\n\n\n188\nGretchen\nStay lingthusiastic! [Music]\n\n\n\n\n\nSave results to data/transcripts.csv:\n\ntranscripts.to_csv('data/transcripts.csv', index=True)\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "3_plot_vowels.html",
    "href": "3_plot_vowels.html",
    "title": "Part 3: Plotting the Vowels",
    "section": "",
    "text": "3.1 Setup\n\n1library(tidyverse)\n2library(magrittr)\n3library(ggtext)\n4library(ggforce)\n5library(ggrepel)\n6library(rcartocolor)\n7library(png)\n8library(patchwork)\n\n9options(dplyr.summarise.inform = FALSE)\n\n\n1\n\nData wrangling (tidyr, dplyr, purrr, stringr), ggplot2 for plotting\n\n2\n\nPipe operator\n\n3\n\nMarkdown/HTML formatting for text in plots\n\n4\n\nEllipsis plots\n\n5\n\nOffset text labels from points\n\n6\n\nColor themes\n\n7\n\nOpen PNG images\n\n8\n\nAdd image to plot\n\n9\n\nDon’t print a message every time summarise() is called on a grouped dataframe.\n\n\n\n\n(Note: this could be done in Python, but I strongly prefer the ggplot package for plotting.)\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  }
]