[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lingthusiasm Vowel Plots",
    "section": "",
    "text": "This is the tutorial website for the Lingthusiasm bonus episode about making vowel plots. I took data from the Lingthusiasm episode recordings and from a recording of the Wells Lexical Set that Gretchen and Lauren made for me:\n\nAnd made stylized versions of the Wells Lexical Set:\n \nAnd of the Lingthusiasm episode data:\n\nPart 1 of the tutorial finds the vowel data to use, Part 2 annotates F1 and F2 in the vowel data, and Part 3 explains how vowel plots work and makes several versions.\nIf you use this tutorial to make your own plots, I’d love to see them! If you have questions about this material, feel to get in touch with me by posting in this GitHub’s discussion page or sending me an email. To see if I’m currently taking freelance contracts for data visualizations or other related tasks, send me an email.\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2024,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2024-02-07},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2024. “Lingthusiasm Vowel Plots.”\nFebruary 7, 2024. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "2_annotate_audio.html",
    "href": "2_annotate_audio.html",
    "title": "Part 2: Annotating the Audio",
    "section": "",
    "text": "There are two sets of data going into these vowel plots:\n\nVowels pulled from the Lingthusiasm episode recordings, which were located in Part 1\nVowels from Gretchen & Lauren recording the Wells lexical set for me\n\nThe next steps are to trim the words out of the episode audio files for #1, then annotate the vowels for both #1 and #2.\n\n2.1 Setup\n\n\"\"\"Part 2 of Lingthusiasm Vowel Plots: Trimming Audio and Getting Vowel Formants.\"\"\"\n\n1import glob\nimport os\n2import pandas as pd\n3from pytube import Playlist, YouTube\n4from pydub import AudioSegment\n5import parselmouth\n\n\n1\n\nFile utilities.\n\n2\n\nDataframes.\n\n3\n\nGetting captions and audio data from YouTube.\n\n4\n\nWorking with audio files.\n\n5\n\nInterface with Praat.\n\n\n\n\nGet video info from Lingthusiasm’s all episodes playlist:\n\nvideo_list = Playlist('https://www.youtube.com/watch?v=xHNgepsuZ8c&' +\n                      'list=PLcqOJ708UoXQ2wSZelLwkkHFwg424u8tG')\n\nGo through each video and download audio (if not already downloaded):\n\ndef get_audio(videos):\n    \"\"\"Download episode audio from Youtube.\"\"\"\n    for url in videos:\n1        video = YouTube(url)\n2        video.bypass_age_gate()\n\n3        title = video.title\n        episode = int(title[:2])\n\n4        audio_file_name = os.path.join(\n            'audio', 'episodes', f'{episode}.mp4')\n5        if not os.path.isfile(audio_file_name):\n            audio_stream = video.streams.get_audio_only()\n            print(f'downloading {episode}')\n            audio_stream.download(filename=audio_file_name)\n\n\nget_audio(video_list)\n\n\n1\n\nGo through the list of video URLs and open each one as a YouTube object.\n\n2\n\nNeed to include this to download data.\n\n3\n\nThe video title is an attribute of the YouTube object, and the episode number is the first word of the title.\n\n4\n\nCreate file name for episode audio.\n\n5\n\nIf file is not already downloaded, select and download the highest-quality audio-only stream.\n\n\n\n\ndownloading 88\ndownloading 87\n\n\n\n\n2.2 Trim Audio from Episodes\nOpen the timestamps data from Part 1:\n\ntimestamps = pd.read_csv( \n    'data/timestamps_annotate.csv',\n    usecols=[\n1        'Vowel', 'Word', 'Speaker', 'Number',\n2        'Episode', 'Start', 'End'\n    ],\n3    dtype={\n        'Vowel': 'category', 'Word': 'category', 'Speaker': 'category',\n        'Number': 'category', 'Episode': 'category',\n        'Start': 'int', 'End': 'int'\n    }\n)\ntimestamps['Speaker'] = timestamps['Speaker'] \\\n    .str.replace('retchen', '').str.replace('auren', '') \\\n4    .astype('category')\n\n\n1\n\nKeep columns specifying word variables.\n\n2\n\nAnd keep columns specifying where audio is.\n\n3\n\nMake all columns categorical variables, except the Start and End times (integers).\n\n4\n\nConvert values in Speaker column from names to initials.\n\n\n\n\nTrim audio for the duration of the caption (with 250ms before and after). This results ~240 audio files each 2-10sec long, each containing a target word.\n\ndef trim_audio(df):\n    \"\"\"Use caption timestamps to trim audio.\"\"\"\n\n1    for i in df.index:\n        episode = df.loc[i, 'Episode']\n        word = df.loc[i, 'Word']\n        speaker = df.loc[i, 'Speaker']\n        count = df.loc[i, 'Number']\n\n2        out_file =  os.path.join(\n            'audio', 'words', f'episode_{word}_{speaker}_{count}.wav')\n        if not os.path.isfile(out_file):\n3            in_file = os.path.join('audio', 'episodes', f'{episode}.mp4')\n            audio = AudioSegment.from_file(in_file, format='mp4')\n4            start = max(df.loc[i, 'Start'] - 250, 0)\n            end = min(len(audio), df.loc[i, 'End'] + 250)\n            clip = audio[start:end]\n            clip.export(out_f=out_file, format='wav')\n\n\ntrim_audio(timestamps)\n\n\n1\n\nGo through dataframe that has the example words to annotate and their timestamps.\n\n2\n\nMake file name for current word, and if it does not already exist…\n\n3\n\nOpen the audio file for the whole episode.\n\n4\n\nTrim the episode audio to start 250 ms after the caption timestamp and end 250 after the caption timestamp; save it.\n\n\n\n\n\n\n2.3 Wells Lexical Set\nThe Wells lexical set is a set of examples for each vowel/diphthong, chosen to be maximally distinguishable and consistent. You can read more about it on Wikipedia and John Wells’ blog. These recordings are going to be more controlled than the vowels pulled from the episode recordings and easier to annotate because they’re spoken more slowly and carefully. This set contains some fairly low-frequency words, which is why there’s not a lot of overlap with the words pulled from the episodes.\n\n\n\nThe Wells lexical set\n\n\n\n1wells_lexical_set = {\n    '\\u0069': 'fleece',   # i\n    '\\u026A': ['kit', 'near'],  # ɪ\n    '\\u025B': ['dress', 'square'],  # ɛ\n    '\\u00E6': ['trap', 'bath'],  # æ\n    '\\u006F': ['force', 'goat'], # o \n    '\\u0075': 'goose',  # u\n    '\\u028A': ['cure', 'foot'],  # ʊ\n    '\\u0254': ['cloth', 'north', 'thought'],  # ɔ\n    '\\u0251': ['lot', 'palm', 'start'],  # ɑ\n    '\\u028C': 'strut'  # ʌ\n}\n\nwells_lexical_set = pd.DataFrame.from_dict(wells_lexical_set, orient='index') \\\n    .rename(columns={0: 'Word'}) \\\n    .explode('Word') \\\n2    .reset_index(names='Vowel')\n\n\n1\n\nDictionary where keys are the IPA vowel unicode and values is word(s).\n\n2\n\nConvert to dataframe with columns for Vowel and Word.\n\n\n\n\nHere’s the full set of words for each vowel:\n\n1word_list = pd.concat([\n2    pd.DataFrame({\n        'List': 'lexicalset',\n        'Vowel': wells_lexical_set['Vowel'],\n        'Word': wells_lexical_set['Word']\n    }),\n3    timestamps[['Vowel', 'Word']].drop_duplicates()\n  ])\n4word_list = word_list.fillna('episode')\n5word_list = word_list.sort_values(by = ['Vowel', 'Word'])\nword_list = word_list.reset_index(drop = True)\n\n6word_list.style.hide()\n\n\n1\n\nCombine word lists from episodes and Wells lexical set.\n\n2\n\nDataframe for Wells lexical set with columns for List, Vowel, and Word.\n\n3\n\nSubset dataframe for episode word list, with columns for Vowel and List.\n\n4\n\nFill the NA values of List with episode.\n\n5\n\nSort and reset index.\n\n6\n\nPrint, not including index.\n\n\n\n\n\n\n\n\n\nList\nVowel\nWord\n\n\n\n\nepisode\ni\nbeat\n\n\nepisode\ni\nbelieve\n\n\nlexicalset\ni\nfleece\n\n\nepisode\ni\npeople\n\n\nlexicalset\no\nforce\n\n\nlexicalset\no\ngoat\n\n\nepisode\nu\nblue\n\n\nlexicalset\nu\ngoose\n\n\nepisode\nu\nthrough\n\n\nepisode\nu\nwho\n\n\nepisode\næ\nbang\n\n\nlexicalset\næ\nbath\n\n\nepisode\næ\nhand\n\n\nepisode\næ\nlaugh\n\n\nlexicalset\næ\ntrap\n\n\nepisode\nɑ\nball\n\n\nepisode\nɑ\nfather\n\n\nepisode\nɑ\nhonorific\n\n\nlexicalset\nɑ\nlot\n\n\nlexicalset\nɑ\npalm\n\n\nlexicalset\nɑ\nstart\n\n\nepisode\nɔ\nbought\n\n\nlexicalset\nɔ\ncloth\n\n\nepisode\nɔ\ncore\n\n\nlexicalset\nɔ\nnorth\n\n\nlexicalset\nɔ\nthought\n\n\nepisode\nɔ\nwrong\n\n\nepisode\nə\namong\n\n\nepisode\nə\nfamous\n\n\nepisode\nə\nsupport\n\n\nepisode\nɛ\nbet\n\n\nlexicalset\nɛ\ndress\n\n\nepisode\nɛ\nguest\n\n\nepisode\nɛ\nsays\n\n\nlexicalset\nɛ\nsquare\n\n\nepisode\nɪ\nbit\n\n\nepisode\nɪ\nfinish\n\n\nlexicalset\nɪ\nkit\n\n\nlexicalset\nɪ\nnear\n\n\nepisode\nɪ\npin\n\n\nepisode\nʊ\ncould\n\n\nlexicalset\nʊ\ncure\n\n\nlexicalset\nʊ\nfoot\n\n\nepisode\nʊ\nfoot\n\n\nepisode\nʊ\nput\n\n\nepisode\nʌ\nanother\n\n\nepisode\nʌ\nbut\n\n\nepisode\nʌ\nfun\n\n\nlexicalset\nʌ\nstrut\n\n\n\n\n\n\n\n2.4 An Interlude in Praat\nNow, we have about 400 audio clips of Gretchen and Lauren saying words that are good examples of each vowel. The vowel data that will actually be going into the plots is F1 and F2 [todo: link to overview of formants]. The easiest way to calculate the vowel formants is using Praat, a software designed for doing phonetic analysis.\nHere’s an example of what that looked like:\n\n\n\nPraat screenshot of Lauren saying “pit.”\n\n\nThe vowel [ɪ] is highlighted in pink, and you can see it’s darker on the spectrogram than the consonants [k] and [t] before and after it. I placed an annotation (the blue lines, which Praat calls a “point tier”) right in the middle of the vowel sound—this one is pretty easy, because Lauren was speaking slowly and without anyone overlapping, so the vowel sound is long and clear. The formants are the lines of red dots, and the popup window is the formant values at the vowel annotation time. We’ll be using F1 (the bottom one) and F2 (the second from the bottom).\nYou can download Praat and see the documentation here. It’s fairly old, so a con is that the interface isn’t necessarily intuitive if you’re used to modern programs, but a pro is that there are a ton of resources available for learning how to use it. Here’s a tutorial about getting started with Praat, and here’s one for recording your own audio and calculating the formants in it.\nAfter going through all of the audio clips, I had a .TextGrid file (Praat’s annotation file format) for each audio clip that includes the timestamp for middle(ish) of the vowel. You can copy formant values manually out of Praat, or you can use Praat scripting to export them to a csv file (see this tutorial, for example). But I prefer to go back to Python instead of wrangling Praat scripting code.\n\n\n2.5 Read Annotation Data\nThere are packages that read Praat TextGrid files, but I kept getting errors. Luckily, the textgrids for these annotations are simple text files, where we only need to extra one variable (the time of the point tier). These two functions do that:\n\ndef get_tier(text):\n    \"\"\"Get annotation info from Praat TextGrid.\"\"\"\n\n1    tg = text.split('class = \"TextTier\"')[1]\n2    tg = tg.splitlines()[1:]\n    tg = pd.Series(tg)\n3    tg = tg.str.partition('=')\n    tg.drop(columns=1, inplace=True)\n    tg.rename(columns={0: 'Name', 2: 'Value'}, inplace=True)\n4    tg['Name'] = tg['Name'].str.strip()\n    tg['Value'] = tg['Value'].str.strip()\n5    tg.set_index('Name', inplace=True)\n\n    return tg['Value'].to_dict()\n\n\ndef get_point_tier_time(t):\n    \"\"\"Get time from TextGrid PointTier.\"\"\"\n\n6    tg = get_tier(t)\n7    time = tg['number']\n8    time = float(time)\n\n    return round(time, 4)\n\n\n1\n\nSection we need in TextGrid files start with this string.\n\n2\n\nSplit string by line breaks and convert to pandas series.\n\n3\n\nSplit into columns by = character, where the first column is the variable name, the second column is = (and gets dropped), and the third column is the variable value.\n\n4\n\nRemove extra whitespace.\n\n5\n\nMake Name into index, so the dataframe can be converted to a dictionary.\n\n6\n\nRead TextGrid file using function defined immediately above.\n\n7\n\nThe variable we want (the timestamp for the PointTier annotation) is called number.\n\n8\n\nConvert the time from character to numeric and round to 4 digits.\n\n\n\n\nThese functions cycle through the list of TextGrid files, extract the point tier times, and put them into a dataframe with the rest of the information for each word:\n\ndef read_textgrid_times(file_list, word_list):\n    \"\"\"Read textgrid files into dataframe.\"\"\"\n\n    tg_times = []\n    for file_name in file_list:\n        with open(file_name, encoding='utf-8') as t_file:\n            t = t_file.read()\n            try:\n1                tg = get_point_tier_time(t)\n            except KeyError:\n                tg = None\n            tg_times.append(tg)\n\n2    df = pd.DataFrame({'File': file_list, 'Vowel_Time': tg_times})\n3    df['File'] = df['File'].str.rpartition('\\\\')[2]\n    df['File'] = df['File'].str.removesuffix('.TextGrid')\n\n4    return textgrid_vars(df, word_list)\n\n\ndef textgrid_vars(df, word_list):\n    \"\"\"Format df of vowel timestamps.\"\"\"\n\n5    df['List'] = df['File'].str.split('_', expand=True)[0]\n    df['Word'] = df['File'].str.split('_', expand=True)[1]\n    df['Speaker'] = df['File'].str.split('_', expand=True)[2]\n    df['Count'] = df['File'].str.split('_', expand=True)[3]\n\n6    df = pd.merge(df, word_list, how='left', on=['Word', 'List'])\n\n7    return df[['List', 'Vowel', 'Word', 'Speaker', 'Count', 'Vowel_Time']]\n\n\n1\n\nTry to get timestamp from PointTier annotation for each TextGrid file, using function defined in previous code chunk.\n\n2\n\nPut results into a dataframe.\n\n3\n\nRemove the path prefix and type suffix from the file names, leaving just the word_speaker_number format.\n\n4\n\nGet other variables from File, using function defined immediately below.\n\n5\n\nWhen File is split by _, List (episode or lexicalset) is the first item, Word is the second item, Speaker (G or L) is the third item, and Count is the fourth item in the resulting list.\n\n6\n\nMerge with the word list dataframe to add a column for Vowel by matching on Word.\n\n7\n\nOrganize.\n\n\n\n\nMake list of TextGrid files and read PointTier times:\n\n1tg_list = glob.glob(os.path.join('audio', 'words', '*.TextGrid'))\n2formants = read_textgrid_times(tg_list, word_list)\n\n3pd.concat([formants.head(), formants.tail()]).style.hide()\n\n\n1\n\nGet a list of all .TextGrid files in the audio/words/ directory.\n\n2\n\nGet the data from each TextGrid file, using functions defined in previous two code chunks.\n\n3\n\nGet the first and last 10 rows, then display as a table, not including row numbers.\n\n\n\n\n\n\n\n\n\nList\nVowel\nWord\nSpeaker\nCount\nVowel_Time\n\n\n\n\nepisode\nə\namong\nG\n1\n1.445500\n\n\nepisode\nə\namong\nG\n2\n2.705600\n\n\nepisode\nə\namong\nG\n3\n3.847700\n\n\nepisode\nə\namong\nG\n4\n2.780400\n\n\nepisode\nə\namong\nG\n5\n2.237000\n\n\nlexicalset\næ\ntrap\nG\n2\n0.363400\n\n\nlexicalset\næ\ntrap\nG\n3\n0.235500\n\n\nlexicalset\næ\ntrap\nL\n1\n0.277300\n\n\nlexicalset\næ\ntrap\nL\n2\n0.221500\n\n\nlexicalset\næ\ntrap\nL\n3\n0.181500\n\n\n\n\n\n\n\n2.6 Calculate Formants\nIt’s possible to export the formants from Praat, but I think it’s easier to use the parselmouth package here, which runs Praat from Python.\n\ndef get_formants(df):\n    \"\"\"Get F1 and F2 at specified time.\"\"\"\n\n    for i in df.index:\n1        file = os.path.join(\n            'audio', 'words',\n            (df.loc[i, 'List'] + '_' + df.loc[i, 'Word'] + '_' +\n            df.loc[i, 'Speaker'] + '_' + df.loc[i, 'Count'] + '.wav')\n        )\n2        audio = parselmouth.Sound(file)\n3        formants = audio.to_formant_burg(\n            time_step=0.01, max_number_of_formants=5\n        )\n4        if 'fleece_L_2' in file:\n            formants = audio.to_formant_burg(\n                time_step=0.01, max_number_of_formants=4\n            )\n5        vowel_time = df.loc[i, 'Vowel_Time']\n6        df.loc[i, 'F1'] = formants.get_value_at_time(1, vowel_time)\n        df.loc[i, 'F2'] = formants.get_value_at_time(2, vowel_time)\n\n    return df\n\n\n1\n\nReconstruct the current audio file name from Word + Speaker + Count variables.\n\n2\n\nUse the parselmouth package to open the audio file.\n\n3\n\nCall Praat via parselmouth and calculate the formants. These are the default settings: every 0.010 seconds, up to 5 formants.\n\n4\n\nBecause of artefacts in the recording, this file needs a limit of 4 formants to identify them correctly.\n\n5\n\nGet the timestamp of the vowel for the current audio file.\n\n6\n\nGet F1 and F2 at the specified time from the parselmouth formant object.\n\n\n\n\nCalculate the formants and summarize the results:\n\n1formants = get_formants(formants)\n\n2pd.concat([formants.head(), formants.tail()]).style.hide()\n\n\n1\n\nCalculate vowel formants for each word, using function defined in previous code chunk.\n\n2\n\nGet the first and last 10 rows, then display as a table, not including row numbers.\n\n\n\n\n\n\n\n\n\nList\nVowel\nWord\nSpeaker\nCount\nVowel_Time\nF1\nF2\n\n\n\n\nepisode\nə\namong\nG\n1\n1.445500\n765.824033\n1259.934246\n\n\nepisode\nə\namong\nG\n2\n2.705600\n602.121127\n1253.818304\n\n\nepisode\nə\namong\nG\n3\n3.847700\n733.394036\n1174.821421\n\n\nepisode\nə\namong\nG\n4\n2.780400\n766.380340\n1372.590268\n\n\nepisode\nə\namong\nG\n5\n2.237000\n625.651118\n1259.707370\n\n\nlexicalset\næ\ntrap\nG\n2\n0.363400\n993.455993\n1672.473430\n\n\nlexicalset\næ\ntrap\nG\n3\n0.235500\n994.743332\n1709.214039\n\n\nlexicalset\næ\ntrap\nL\n1\n0.277300\n915.395294\n1516.026185\n\n\nlexicalset\næ\ntrap\nL\n2\n0.221500\n903.045126\n1466.625440\n\n\nlexicalset\næ\ntrap\nL\n3\n0.181500\n987.048619\n1601.687398\n\n\n\n\n\n\nformants.groupby(['Vowel', 'Speaker']) \\\n    .agg({'F1': ['min', 'mean', 'max'], 'F2': ['min', 'mean', 'max']}) \\\n    .style \\\n1    .format(precision=0)\n\n\n1\n\nGroup the data by Vowel and Speaker, then calculate the min, mean, and max of F1 and F2 for each Vowel + Speaker combination. Print the results as a table, with values rounded to whole numbers.\n\n\n\n\n\n\n\n\n\n \n \nF1\nF2\n\n\n \n \nmin\nmean\nmax\nmin\nmean\nmax\n\n\nVowel\nSpeaker\n \n \n \n \n \n \n\n\n\n\ni\nG\n244\n353\n436\n2284\n2566\n2882\n\n\nL\n355\n420\n512\n2240\n2635\n2924\n\n\no\nG\n505\n592\n691\n781\n1087\n1491\n\n\nL\n497\n583\n665\n727\n1011\n1269\n\n\nu\nG\n337\n397\n536\n977\n1589\n2050\n\n\nL\n293\n396\n481\n1416\n1834\n2140\n\n\næ\nG\n586\n859\n1141\n1438\n1802\n2187\n\n\nL\n784\n914\n1015\n1236\n1619\n2127\n\n\nɑ\nG\n513\n721\n884\n753\n1024\n1439\n\n\nL\n443\n726\n900\n672\n1123\n1347\n\n\nɔ\nG\n338\n695\n900\n604\n1016\n1214\n\n\nL\n483\n600\n766\n609\n925\n1273\n\n\nə\nG\n404\n575\n766\n1175\n1505\n1893\n\n\nL\n384\n621\n935\n1208\n1513\n1863\n\n\nɛ\nG\n512\n721\n883\n1629\n1827\n2093\n\n\nL\n489\n669\n803\n1670\n1954\n2327\n\n\nɪ\nG\n367\n483\n624\n1675\n2098\n2693\n\n\nL\n275\n452\n533\n1940\n2383\n2797\n\n\nʊ\nG\n316\n521\n661\n962\n1365\n1940\n\n\nL\n407\n488\n633\n950\n1344\n2111\n\n\nʌ\nG\n528\n726\n971\n1239\n1470\n1719\n\n\nL\n528\n728\n970\n1140\n1382\n1799\n\n\n\n\n\nSave results as data/formants.csv:\n\nformants.to_csv('data/formants.csv', index=False)\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2024,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2024-02-07},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2024. “Lingthusiasm Vowel Plots.”\nFebruary 7, 2024. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "1_find_words.html",
    "href": "1_find_words.html",
    "title": "Part 1: Finding Vowels to Plot",
    "section": "",
    "text": "There are going to be two sources of data, one more naturalistic and one more controlled:\n\nVowels pulled from the Lingthusiasm episode recordings\nVowels from Gretchen & Lauren recording the Wells lexical set\n\nThe first steps are to find the words for #1, and we’ll come back to #2 later.\n\n1.1 Setup\n\n\"\"\"Part 1 of Lingthusiasm Vowel Plots: Finding Vowels to Annotate.\"\"\"\n\n1import re\n2import pandas as pd\n3import requests\nfrom bs4 import BeautifulSoup\n4from thefuzz import process\n5from pytube import Playlist, YouTube\n\n\n1\n\nRegex functions.\n\n2\n\nDataframe functions.\n\n3\n\nScraping data from webpages.\n\n4\n\nFuzzy string matching.\n\n5\n\nGetting captions and audio from YouTube.\n\n\n\n\n(Note: All of this could also be done in R, but I’ve done it in Python here primarily because there are Python packages that can get data from YouTube without having to set up anything on the YouTube API side. Here, all you need to do is install the package.)\n\n\n1.2 Get Transcript Text & Speakers\n\n1.2.1 List Episodes\nThe Lingthusiasm website has a page listing all of the available transcripts. Step 1 is to load that page and get the list of URLs to the transcripts. (They have similar but not identical structures, so it’s easiest to read the list from the website instead of trying to construct them here.)\nThis function uses the BeautifulSoup package to return an HTML object and a text string for a URL:\n\ndef get_html_text(url):\n    \"\"\"Use BeautifulSoup to get the webpage text from the URL.\"\"\"\n\n1    resp = requests.get(url, timeout=1000)\n2    html = BeautifulSoup(resp.text, 'html.parser')\n\n3    return {'html': html, 'text': html.get_text()}\n\n\n1\n\nConnect to webpage.\n\n2\n\nLoad the HTML data from the webpage.\n\n3\n\nReturn the HTML data object and the text from the HTML data object.\n\n\n\n\nThis function uses BeautifulSoup to filter just the transcript URLs from the HTML data:\n\ndef get_transcript_links(url):\n    \"\"\"Get URLs to episode transcripts from HTML page.\"\"\"\n\n1    html = get_html_text(url)['html']\n2    url_objs = html.find_all('a')\n3    urls = [l.get('href') for l in url_objs]\n4    urls = pd.Series(data=urls, name='URL', dtype='string')\n5    urls = urls[urls.str.contains('transcript')]\n6    urls = urls[::-1]\n\n7    return urls.reset_index(drop=True)\n\n\n1\n\nGet HTML data from webpage using function defined above.\n\n2\n\nFilter using the a tag to get all of the link items.\n\n3\n\nGet the href item from each a tag item, which is the text of the URL.\n\n4\n\nConvert from list to pandas series.\n\n5\n\nFilter to only include URLs including the word transcript.\n\n6\n\nSort to have earliest episodes first.\n\n7\n\nReturn with index (row numbers) reset.\n\n\n\n\nThere are 84 episodes available (as of early October 2023):\n\n1transcript_urls = get_transcript_links('https://lingthusiasm.com/transcripts')\n\n2transcript_urls.head().to_frame().style\n\n\n1\n\nGet the URLs for the episode transcripts from the table of contents page, using the functions defined above.\n\n2\n\nTake the first 10 rows of the resulting list and print it as a nice table.\n\n\n\n\n\n\n\n\n\n \nURL\n\n\n\n\n0\nhttps://lingthusiasm.com/post/155357756341/transcript-lingthusiasm-episode-1-speaking-a\n\n\n1\nhttps://lingthusiasm.com/post/156181768226/transcript-lingthusiasm-episode-2-pronouns\n\n\n2\nhttps://lingthusiasm.com/post/157167562811/transcript-lingthusiasm-episode-3-arrival-of-the\n\n\n3\nhttps://lingthusiasm.com/post/157268108811/transcript-lingthusiasm-episode-4-inside-the-word\n\n\n4\nhttps://lingthusiasm.com/post/158014366301/transcript-lingthusiasm-episode-5-colour-words\n\n\n\n\n\nOnly keep episodes 1-84, for consistency replicating this code later:\n\ntranscript_urls = transcript_urls[:84]\n\n\n\n1.2.2 Scrape Transcript Text\nNow we can download transcripts, which are split into turns and labelled by speaker.\nSplitting out some sub-tasks into separate methods, this function gets the episode number (following episode-) as an integer from the URL:\n\ndef transcript_number_from_url(url):\n    \"\"\"Find transcript number in URL.\"\"\"\n\n1    index_episode = url.find('episode-')\n2    cur_number = url[index_episode + 8:]\n3    index_end = cur_number.find('-')\n    if index_end != -1:\n        cur_number = cur_number[:index_end]\n\n4    return int(cur_number)\n\n\n1\n\nFind location of episode-, since episode number is immediately following it.\n\n2\n\nSubset URL starting 8 characters after start of episode- string, which is immediately after it.\n\n3\n\nMost of the transcript URLs have more text after the number. If so, trim to just keep the number.\n\n4\n\nReturn episode number converted from string to integer.\n\n\n\n\nThe text returned from the transcript pages has information at the top and bottom that we don’t need. \"This is a transcript for\" marks the start of the transcript, and \"This work is licensed under\" marks the end, so subset the transcript dataframe rows to only include that section:\n\ndef trim_transcript(df):\n    \"\"\"Find start and end of transcript text.\"\"\"\n\n    start_index = df.find('This is a transcript for')\n    end_index = df.find('This work is licensed under')\n\n    return df[start_index:end_index]\n\nThis function cleans up the text column so it plays a bit nicer with Excel CSV export:\n\ndef clean_text(l):\n    \"\"\"Clean text column so it opens correctly as Excel CSV.\"\"\"\n\n1    l = l.strip()\n2    l = l.replace('\\u00A0', ' ').replace('–', '--').replace('…', '...')\n3    l = re.sub('“|”|‘|’', \"'\", l)\n\n4    return ' '.join(l.split())\n\n\n1\n\nRemove leading and trailing whitespaces.\n\n2\n\nReplace non-breaking spaces, en dashes, and ellipses.\n\n3\n\nReplace slanted quotes.\n\n4\n\nRemove double spaces between words.\n\n\n\n\nAfter a bit of trial and error, it’s easiest to split on the speaker names, since the paragraph formatting isn’t identical across all the pages:\n\nspeaker_names = [\n    'Gretchen', 'Lauren', 'Ake', 'Bona', 'Ev', 'Fei Ting', 'Gabrielle',\n    'Jade', 'Hannah', 'Hilaria', 'Janelle', 'Kat', 'Kirby', 'Lina', 'Nicole',\n    'Pedro', 'Randall', 'Shivonne', 'Suzy'\n]\n1speaker_regex = '(' + ':)|('.join(speaker_names) + ':)'\n\n\n1\n\nRegex looks like (Name1:)|(Name2:) etc. Surrounding names in parentheses makes it a capture group, so the names are included in the list of split items instead of dropped.\n\n\n\n\nThis function puts it all together to read the transcripts into one dataframe:\n\ndef get_transcripts(urls):\n    \"\"\"Get transcript text from URLs and format into dataframe.\"\"\"\n\n1    df = []\n    for l in urls:\n2        cur_text = get_html_text(l)['text']\n3        cur_text = trim_transcript(cur_text)\n4        cur_lines = re.split(speaker_regex, cur_text)\n5        cur_lines = [l for l in cur_lines if l is not None]\n6        cur_lines = cur_lines[1:]\n7        speakers = cur_lines[::2]\n        turns = cur_lines[1::2]\n8        cur_df = pd.DataFrame({\n            'Episode': transcript_number_from_url(l),\n            'Speaker': speakers,\n            'Text': [clean_text(line) for line in turns],\n        })\n        cur_df['Speaker'] = cur_df['Speaker'].str.removesuffix(':')\n9        cur_df['Turn'] = cur_df.index + 1\n10        cur_df = cur_df.set_index(['Episode', 'Turn'], drop=True)\n11        df.append(cur_df)\n\n12    df = pd.concat(df)\n13    df['Speaker'] = df['Speaker'].astype('category')\n    df['Text'] = df['Text'].astype('string')\n\n    return df\n\n\n1\n\nMake list to store results.\n\n2\n\nRead text string (vs HTML object) from URL, using function defined above.\n\n3\n\nTrim to only include transcript section, using function defined above.\n\n4\n\nSplit string into list with one item for each speaker turn, using regex defined above.\n\n5\n\nDrop items in list that are None (which are included because of capture group syntax).\n\n6\n\nDrop the initial “This is a transcript for…” line.\n\n7\n\nNow all the odd items are speaker labels and even items are turn text. [::2] is every 2nd item starting at 0, and [1::2] is every 2nd item starting at 1.\n\n8\n\nClean up strings and put everything into a dataframe.\n\n9\n\nMake column for turn number, which is index + 1.\n\n10\n\nSet Episode and Turn columns as indices.\n\n11\n\nAdd to list of parsed episodes.\n\n12\n\nCombine list of dataframes into one dataframe. This works because the Episode + Turn index combination is unique.\n\n13\n\nSet datatypes explicitly, to avoid warnings later.\n\n\n\n\nThis takes a minute or so to run:\n\n1transcripts = get_transcripts(transcript_urls)\n\n2pd.concat([transcripts.head(), transcripts.tail()]).style\n\n\n1\n\nRun get_transcripts() on the lists of links to each transcript, defined above.\n\n2\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \n \nSpeaker\nText\n\n\nEpisode\nTurn\n \n \n\n\n\n\n1\n1\nLauren\nWelcome to Lingthusiasm! A podcast that's enthusiastic about linguistics. I'm Lauren Gawne.\n\n\n2\nGretchen\nAnd I'm Gretchen McCulloch. And today we're going to be talking about universal language and why it doesn't work. But first a little bit about what Lingthusiasm is.\n\n\n3\nLauren\nSo I guess the important thing is to unpack 'a podcast that's enthusiastic about linguistics'.\n\n\n4\nGretchen\nYeah! We chose our tagline because we're here to explore interesting things that language has to offer and especially what looking at language from a linguistics perspective can tell us about how language works.\n\n\n5\nLauren\nBoth of us have blogs where we do a fair amount of that, but that's a fairly solitary enterprise and we wanted to take the opportunity to have more of a conversation -- something that's a little less disembodied\n\n\n84\n184\nLauren\nAh, well, that's okay, because now we're at the end of this episode, I'm pointing to everyone. [Music]\n\n\n185\nLauren\nFor more Lingthusiasm and links to all the things pointed to in this episode, go to lingthusiasm.com. You can listen to us on Apple Podcasts, Google Podcasts, Spotify, SoundCloud, YouTube, or wherever else you get your podcasts. You can follow @lingthusiasm on Twitter, Facebook, Instagram, and Tumblr. You can get 'Etymology isn't Destiny' on t-shirts and tote bags and lots of other items, and aesthetic IPA posters, and other Lingthusiasm merch at lingthusiasm.com/merch. I tweet and blog as Superlinguo.\n\n\n186\nGretchen\nI can be found as @GretchenAMcC on Twitter, my blog is AllThingsLinguistic.com, and my book about internet language is called Because Internet. Lingthusiasm is able to keep existing thanks to the support of our patrons. If you wanna get an extra Lingthusiasm episode to listen to every month, our entire archive of bonus episodes to listen to right now, or if you just wanna help keep the show running ad-free, go to patreon.com/lingthusiasm or follow the links from our website. Patrons can also get access to our Discord chatroom to talk with other linguistics fans and be the first to find out about new merch and other announcements. Recent bonus topics include interviews with Sarah Dopierala and Martha Tsutsui-Billins about their own linguistic research and their work on Lingthusiasm and a very special Lingthusiasmr episode where we read The Harvard Sentences to you [ASMR voice] in a calm, soothing voice. [Regular voice] Can't afford to pledge? That's okay, too. We also really appreciate it if you can recommend Lingthusiasm to anyone in your life who's curious about language.\n\n\n187\nLauren\nLingthusiasm is created and produced by Gretchen McCulloch and Lauren Gawne. Our Senior Producer is Claire Gawne, our Editorial Producer is Sarah Dopierala, our Production Assistant is Martha Tsutsui-Billins, and our Editorial Assistant is Jon Kruk. Our music is 'Ancient City' by The Triangles.\n\n\n188\nGretchen\nStay lingthusiastic! [Music]\n\n\n\n\n\nSave results to data/transcripts.csv:\n\ntranscripts.to_csv('data/transcripts.csv', index=True)\n\n\n\n\n1.3 Find Words For Each Vowel\n\n1.3.1 Which Words To Use?\nThese words aren’t all as controlled as you’d probably want for an experiment, but they’re high frequency enough that there are multiple tokens for each one in the episodes.\n\n1word_list = {\n2    '\\u0069': {   # i\n3        'beat': r'\\bbeat\\b',\n        'believe': r'\\bbelieve\\b',\n        'people': r'\\bpeople\\b'},\n    '\\u026A': {  # ɪ\n        'bit': r'\\bbit\\b',\n4        'finish': r'\\bfinish',\n        'pin': r'\\bpin\\b'},\n    '\\u025B': {  # ɛ\n        'bet': r'\\bbet\\b',\n5        'guest': r'\\bguest',\n        'says': r'\\bsays\\b'},\n    '\\u00E6': {  # æ\n        'bang': r'\\bbang\\b',\n        'hand': r'\\bhand\\b',\n        'laugh': r'\\blaugh\\b'},\n    '\\u0075': {  # u\n        'blue': r'\\bblue\\b',\n        'through': r'\\bthrough\\b',\n        'who': r'\\bwho\\b'},\n    '\\u028A': {  # ʊ\n        'could': r'\\bcould\\b',\n        'foot': r'\\bfoot\\b',\n        'put': r'\\bput\\b'},\n    '\\u0254': {  # ɔ\n        'bought': r'\\bbought\\b',\n        'core': r'\\bcore\\b',\n        'wrong': r'\\bwrong\\b'},\n    '\\u0251': {  # ɑ\n        'ball': r'ball\\b|\\bballs\\b',\n        'father': r'\\bfather\\b',\n6        'honorific': r'\\bhonorific'},\n    '\\u028C': {  # ʌ\n        'another': r'\\banother\\b',\n        'but': r'\\bbut\\b',\n        'fun': r'\\bfun\\b'},\n    '\\u0259': {  # ə\n        'among': r'\\bamong\\b',\n        'famous': r'\\bfamous\\b',\n        'support': r'\\bsupport\\b'}\n}\n\n\n1\n\nSet up a nested dictionary, where the outer keys are the IPA vowels, the inner keys are the words, and the values are the regex strings for the words.\n\n2\n\n\\u makes it a unicode string.\n\n3\n\n\\b means search at word boundaries, and only a few words here also include suffixes (e.g., only return results for bit, but return results for finish, finishes, and finishing).\n\n4\n\nAlso get finishes/finishing.\n\n5\n\nAlso get guests.\n\n6\n\nAlso get honorifics. Using honorific instead of honor because there’s an episode about honorifics.\n\n\n\n\n\n\n1.3.2 Find Words In Transcript\nThis function goes through the transcript dataframe and finds turns by Gretchen or Lauren containing the target words:\n\ndef filter_for_words(words, df_all):\n    \"\"\"Find turns by Gretchen/Lauren that contain the target words.\"\"\"\n\n1    df_gl = df_all[\n        (df_all['Speaker'] == \"Gretchen\") |\n        (df_all['Speaker'] == \"Lauren\")\n    ]\n\n2    df_words = pd.DataFrame()\n3    for vowel, examples in words.items():\n4        for word, reg in examples.items():\n5            has_word = df_gl[\n                df_gl['Text'].str.contains(pat=reg, flags=re.IGNORECASE)\n            ]\n6            has_word.insert(0, 'Vowel', vowel)\n            has_word.insert(1, 'Word', word)\n            has_word.set_index(['Word', 'Vowel'], inplace=True, append=True)\n            has_word.index = has_word.index.reorder_levels(\n                ['Vowel', 'Word', 'Episode', 'Turn'])\n\n7            df_words = pd.concat([df_words, has_word])\n\n8    return df_words.sort_index()\n\n\n1\n\nFilter to only include rows where Speaker is Gretchen or Lauren, not guests.\n\n2\n\nCreate dataframe to store results.\n\n3\n\nLoop through outer layer of dictionary, where keys are vowels and values are lists of example words.\n\n4\n\nLoop through inner dictionaries, where keys are the words and values are the regexes.\n\n5\n\nFilter to only include rows where Text column matches word regex. re.IGNORECASE means the search is not case-sensitive.\n\n6\n\nMake columns for the current Vowel and Word, then add them to the index. The results dataframe is now indexed by unique combinations of Vowel, Word, Episode, and Turn.\n\n7\n\nAdd results for current word to dataframe of all results.\n\n8\n\nSorting the dataframe indices makes search faster later, and pandas will throw warnings if you search this large-ish dataframe before sorting.\n\n\n\n\nNext, this function trims the conversation turn around the target word, so it can be matched to the caption timestamps later:\n\ndef trim_turn(df):\n    \"\"\"Find location of target word in conversation turn and subset -/+ 25 characters around it.\"\"\"\n\n1    df['Text_Subset'] = pd.Series(dtype='string')\n\n2    for vowel, word, episode, turn in df.index:\n3        text_full = df.loc[vowel, word, episode, turn]['Text']\n4        word_loc = re.search(re.compile(word, re.IGNORECASE), text_full)\n        word_loc = word_loc.span()[0]\n5        sub_start = max(word_loc - 25, 0)\n6        sub_end = min(word_loc + 25, len(text_full))\n7        text_sub = text_full[sub_start:sub_end]\n        df.loc[(vowel, word, episode, turn), 'Text_Subset'] = str(text_sub)\n\n    return df\n\n\n1\n\nMake a new column for the subset text, so it can be specified as a string. If you insert values later without initializing the column as a string, pandas will throw warnings.\n\n2\n\nGo through each row of the dataframe with all the transcript turns by Gretchen or Lauren that contain target words.\n\n3\n\nUse index values to get the value of Text.\n\n4\n\nSearch for the current row’s Word in the current row’s Text (again case-insensitive). This returns a search object, and the first item in that tuple is the location of the first letter of Word in Text.\n\n5\n\nStart index of Text_Subset is 25 characters before the start of the Word, or the beginning of the string, whichever is larger.\n\n6\n\nEnd index of Text_Subset is 25 characters after the start of the Word, or the end of the string, whichever is smaller.\n\n7\n\nInsert Text_Subset into dataframe.\n\n\n\n\nSubset the transcripts dataframe to only include turns by Gretchen/Lauren that contain a target word:\n\n1transcripts_subset = filter_for_words(word_list, transcripts)\ntranscripts_subset = trim_turn(transcripts_subset)\n\n2pd.concat([transcripts_subset.head(), transcripts_subset.tail()]).style\n\n\n1\n\nRun the two functions just defined on the dataframe of all the transcripts, to filter to only include turns by Gretchen or Lauren that contain the target words, then trim the text of each turn around the target word.\n\n2\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \n \n \n \nSpeaker\nText\nText_Subset\n\n\nVowel\nWord\nEpisode\nTurn\n \n \n \n\n\n\n\ni\nbeat\n1\n119\nGretchen\nno, but you like, you just beat the potato but you don't necessarily deep-fried it you could just pan-fry it.\n, but you like, you just beat the potato but you d\n\n\n12\n86\nGretchen\nYeah, we make it into something like what we have. But another example is, so the sound /ɪ/ as in 'bit,' is not a sound that French has. So you'll have French speakers saying -- well, it kind of has it, but not really -- so if you have something like 'beat' versus 'bit,' for French speakers those are both just kind of 'beat.'\nyou have something like 'beat' versus 'bit,' for F\n\n\n18\n68\nGretchen\nAnd so you have your, like, duh-DUH beat, your iamb, with weak-strong --\nhave your, like, duh-DUH beat, your iamb, with wea\n\n\n100\nGretchen\nAnd the other translators tend to render it in prose, or in, like, shortened lines, but without paying attention to that beat in the same sort of way.\npaying attention to that beat in the same sort of\n\n\n22\n208\nLauren\nYep. I've already used the word 'proximal' in this episode so you're gonna beat me.\nepisode so you're gonna beat me.\n\n\nʌ\nfun\n82\n127\nGretchen\nYes, we did. Because it's sometimes when you know that you're gonna need a whole bunch of examples in a text or an episode, it's fun to theme them so that you're not just reaching for the same -- like I think we used examples like 'I like cake' a lot, or like, 'I eat ice cream.' A lot of our examples are about ice cream and cake, which is fun. I mean, we do like both of these things, but sometimes, for an episode that's gonna be really example heavy, using horses and farmyard animals and stuff is a fun way to make it a little more distinct from other episodes.\ntext or an episode, it's fun to theme them so that\n\n\n137\nGretchen\nThat was a deliberate choice back in the day. That was a style guide thing. Undoing that, now that it's become this unconscious thing that people are still doing, is more challenging. The fun thing, I guess, about how a lot of example sentences in old school syntax papers use 'John' and 'Mary' and 'Bill' is that there is someone who took a bunch of example sentences from papers since the refer to the same people and stitched them together into a single narrative about the adventures of John and Mary and Bill.\nis more challenging. The fun thing, I guess, about\n\n\n83\n1\nGretchen\nWelcome to Lingthusiasm, a podcast that's enthusiastic about linguistics! I'm Gretchen McCulloch. I'm here with Dr. Pedro Mateo Pedro who's an Assistant Professor at the University of Toronto, Canada, a native speaker of Q'anjob'al, and a learner of Kaqchikel. Today, we're getting enthusiastic about kids acquiring Indigenous languages. But first, some announcements. We love looking up whether two words that look kind of similar are actually historically related, but the history of a word doesn't have to define how it's used today. To celebrate how we can grow up to be more than we ever expected, we have new merch that says, 'Etymology isn't Destiny.' Our artist, Lucy Maddox, has made 'Etymology isn't Destiny' into a swoopy, cursive design with a fun little destiny star on the dot of the eye, available in black, white, and my personal favourite, rainbow gradient. This design is available on lots of different colours and styles of shirts. We've got hoodies, tank tops, t-shirts in classic fit, relaxed fit, curved fit -- plus mugs, notebooks, stickers, water bottles, zipper pouches. You know, if it's on Redbubble, we might've put 'Etymology isn't Destiny' on it. We also have tons of other lingthusiastic merch available in our merch store at lingthusiasm.com/merch. I have to say, it makes a great gift to give to a linguistics enthusiast in your life or to request as a gift if you are that linguistics enthusiast. We also wanna give a special shoutout to our aesthetic redesign of the International Phonetic Alphabet. Last year, we reorganised the classic IPA chart to have colours and have little cute circles and not just be boring grey lines of boxes and to even more elegantly represent the principle that the location of the symbols and rows and columns represents the place and degree of constriction in the mouth. I think it looks really cool. It's also a fun little puzzle to sit there and figure out which of the specific circles around different things stands for what. We've now made this aesthetic IPA chart redesign available on lots more merch options, including several different sizes of posters from small ones you can put on a corkboard to large ones you can put up in your hallway. They look really, really good, especially if you have some sort of office-y space that needs to be decorated. Plus, it's on tote bags and notebooks and t-shirts. If you want everyone you meet to know that you're a giant linguistics nerd, you can take them to conferences and use them to start nerdy conversations with people. If you like the idea of linguistics merch but none of ours so far is quite hitting your aesthetic, or if there's an item that Redbubble sells that you think one of our existing designs would look good on, we've added quite a few merch items in response to people's requests over the years, so we'd love to know where the gaps still are and keep an eye on lingthusiasm.com/merch. Our most recent bonus episode was a behind-the-scenes interview with Sarah Dopierala, who you may recognise as a name from the end credits, about what it's like doing transcripts from a linguistics perspective and her life generally as a linguistics grad student. You can go to patreon.com/lingthusiasm to get access to all of the many bonus episodes and to help Lingthusiasm keep running. [Music]\ny, cursive design with a fun little destiny star o\n\n\n84\n167\nGretchen\nI find this a fun and interesting way of thinking about meaning because the way that I think we're sometimes introduced to meaning in a formal context is through dictionary definitions that give you a description of a cat that says something like, 'A cat is a furry creature with four legs, a long tail, purrs,' whatever other attributes you wanna assign to a cat. But in practice, I didn't learn the word 'cat' by looking it up in a dictionary or having a description provided to me. I learned the word 'cat' by having a bunch of cats pointed out to me. Sometimes, a cat has three legs or is one of those weird hairless cats.\nI find this a fun and interesting way o\n\n\n173\nGretchen\nWe did a whole bonus episode about meaning and the ways that we interact with it talking about the 'Is a pizza a sandwich?' meme, which I would highly recommend because I think it's a very fun bonus episode. It turns out that this is actually how meaning works for most people in context. You see a bunch of examples of cats or birds or chairs or whatever. You come up with a sense of what's in the cluster based on generalising from those examples rather than by having an exact list of parameters because some stuff is an exception to those parameters, and it's still a point in the cluster.\nause I think it's a very fun bonus episode. It tur\n\n\n\n\n\nHere’s how many tokens we have for each speaker and word:\n\ntranscripts_subset \\\n    .value_counts(['Vowel', 'Word', 'Speaker'], sort=False) \\\n    .to_frame('Tokens') \\\n    .reset_index(drop=False) \\\n    .style \\\n1    .hide()\n\n\n1\n\nCount the number of items in each combination of Vowel, Word, and Speaker. Convert those results to a dataframe and call the column of counts Tokens. Change Vowel, Word, and Speaker from indices to columns. Print the results as a table, not including row numbers.\n\n\n\n\n\n\n\n\n\nVowel\nWord\nSpeaker\nTokens\n\n\n\n\ni\nbeat\nGretchen\n10\n\n\ni\nbeat\nLauren\n8\n\n\ni\nbelieve\nGretchen\n25\n\n\ni\nbelieve\nLauren\n23\n\n\ni\npeople\nGretchen\n981\n\n\ni\npeople\nLauren\n679\n\n\nu\nblue\nGretchen\n29\n\n\nu\nblue\nLauren\n11\n\n\nu\nthrough\nGretchen\n123\n\n\nu\nthrough\nLauren\n120\n\n\nu\nwho\nGretchen\n512\n\n\nu\nwho\nLauren\n329\n\n\næ\nbang\nGretchen\n4\n\n\næ\nbang\nLauren\n2\n\n\næ\nhand\nGretchen\n48\n\n\næ\nhand\nLauren\n31\n\n\næ\nlaugh\nGretchen\n7\n\n\næ\nlaugh\nLauren\n5\n\n\nɑ\nball\nGretchen\n15\n\n\nɑ\nball\nLauren\n11\n\n\nɑ\nfather\nGretchen\n9\n\n\nɑ\nfather\nLauren\n7\n\n\nɑ\nhonorific\nGretchen\n4\n\n\nɑ\nhonorific\nLauren\n5\n\n\nɔ\nbought\nGretchen\n5\n\n\nɔ\nbought\nLauren\n4\n\n\nɔ\ncore\nGretchen\n7\n\n\nɔ\ncore\nLauren\n2\n\n\nɔ\nwrong\nGretchen\n49\n\n\nɔ\nwrong\nLauren\n21\n\n\nə\namong\nGretchen\n18\n\n\nə\namong\nLauren\n6\n\n\nə\nfamous\nGretchen\n19\n\n\nə\nfamous\nLauren\n18\n\n\nə\nsupport\nGretchen\n42\n\n\nə\nsupport\nLauren\n24\n\n\nɛ\nbet\nGretchen\n13\n\n\nɛ\nbet\nLauren\n9\n\n\nɛ\nguest\nGretchen\n22\n\n\nɛ\nguest\nLauren\n3\n\n\nɛ\nsays\nGretchen\n93\n\n\nɛ\nsays\nLauren\n40\n\n\nɪ\nbit\nGretchen\n267\n\n\nɪ\nbit\nLauren\n242\n\n\nɪ\nfinish\nGretchen\n8\n\n\nɪ\nfinish\nLauren\n11\n\n\nɪ\npin\nGretchen\n18\n\n\nɪ\npin\nLauren\n5\n\n\nʊ\ncould\nGretchen\n444\n\n\nʊ\ncould\nLauren\n174\n\n\nʊ\nfoot\nGretchen\n13\n\n\nʊ\nfoot\nLauren\n6\n\n\nʊ\nput\nGretchen\n250\n\n\nʊ\nput\nLauren\n116\n\n\nʌ\nanother\nGretchen\n239\n\n\nʌ\nanother\nLauren\n130\n\n\nʌ\nbut\nGretchen\n1785\n\n\nʌ\nbut\nLauren\n1049\n\n\nʌ\nfun\nGretchen\n187\n\n\nʌ\nfun\nLauren\n57\n\n\n\n\n\nGretchen and Lauren each say all of the words more than once, and most of the words have 5+ tokens to pick from.\n\n\n\n1.4 Get Timestamps\nThe transcripts on the Lingthusiasm website have all the speakers labelled, but no timestamps, and the captions on YouTube have timestamps, but few speaker labels. To find where in the episodes the token words are, we’ll have to combine them.\nWe’re going to be getting the caption data and audio from Lingthusiasm’s “all episodes” playlist, using the pytube package.\n\nvideo_list = Playlist('https://www.youtube.com/watch?v=xHNgepsuZ8c&' +\n                      'list=PLcqOJ708UoXQ2wSZelLwkkHFwg424u8tG')\n\nThis function uses the YouTube playlist link to download captions for each video:\n\ndef get_captions(videos):\n    \"\"\"Get and format captions from YouTube URLs.\"\"\"\n1    df = pd.DataFrame()\n\n2    for url in videos:\n        video = YouTube(url)\n3        video.bypass_age_gate()\n\n4        title = video.title\n        number = int(title[:2])\n\n5        caps = caption_version(video.captions)\n        try:\n6            caps = parse_captions(caps.xml_captions)\n        except AttributeError:\n            caps = pd.DataFrame()\n\n7        caps.insert(0, 'Episode', number)\n8        df = pd.concat([df, caps])\n\n9    df = df[['Episode', 'Text', 'Start', 'End']]\n    df[['Start', 'End']] = df[['Start', 'End']].astype('float32')\n\n    return df.sort_values(['Episode', 'Start'])\n\n\n1\n\nStart dataframe for results.\n\n2\n\nGo through the list of video URLs and open each one as a YouTube object.\n\n3\n\nNeed to include this to access captions.\n\n4\n\nThe video title is an attribute of the YouTube object, and the episode number is the first word of the title.\n\n5\n\nLoad captions, which returns a dictionary with the language name as the key and the captions and the values. Select which English version to use (function defined below).\n\n6\n\nConvert XML data to dataframe (function defined below). If this doesn’t work, make an empty placeholder dataframe.\n\n7\n\nAdd Episode column (integer).\n\n8\n\nAdd results from current video to dataframe of all results.\n\n9\n\nReturn dataframe of all captions, after organizing columns and sorting rows by episode number then time.\n\n\n\n\nNow, some helper functions to select which caption version to download and then reformat the data from XML to a dataframe.\nCaption data is most commonly formatted as XML or SRT, which is a text format, but not easily convertible to a dataframe.\n\n1def caption_version(cur_captions):\n    \"\"\"Select which version of the captions to use (formatting varies).\"\"\"\n\n    if 'en' in cur_captions.keys():\n        return cur_captions['en']\n    elif 'en-GB' in cur_captions.keys():\n        return cur_captions['en-GB']\n    elif 'a.en' in cur_captions.keys():\n        return cur_captions['a.en']\n\n\ndef caption_times(xml):\n    \"\"\"Find timestamps in XML data.\"\"\"\n\n2    lines = pd.Series(xml.split('&lt;p')[1:], dtype='string')\n3    df = lines.str.extract(r'(?P&lt;Start&gt;t=\"[0-9]*\")')\n4    df['Start'] = df['Start'].str.removeprefix('t=\"').str.removesuffix('\"')\n    df['Start'] = df['Start'].astype('int')\n\n    return df\n\n\ndef caption_text(xml):\n    \"\"\"Find text and filter out extra tags in XML data.\"\"\"\n\n5    lines = pd.Series(xml.split('&lt;p')[1:], dtype='string')\n6    tags = r'\"[0-9]*\"|t=|d=|w=|a=|ac=|&lt;/p&gt;|&lt;/s&gt;|&lt;s\\s|&gt;\\s|&gt;|&lt;/body|&lt;/timedtext'\n7    texts = lines.str.replace(tags, '', regex=True)\n    texts = texts.str.replace('&#39;', '\\'') \\\n        .str.replace('&quot;', \"'\") \\\n        .str.replace('&lt;', '[') \\\n        .str.replace('&gt;', ']') \\\n8        .str.replace('&amp;', ' and ')\n    texts = [clean_text(line) for line in texts.to_list()]\n\n9    return pd.DataFrame({'Text': texts}, dtype='string')\n\n\ndef parse_captions(xml):\n    \"\"\"Combine timestamp rows and text rows.\"\"\"\n\n10    times = caption_times(xml)\n    texts = caption_text(xml)\n    df = times.join(texts)\n11    df = df[df['Text'].str.contains(r'[a-z]')]\n    df = df.reset_index(drop=True)\n12    df['End'] = df['Start'].shift(-1) - 1\n\n    return df\n\n\n1\n\nPrefer en captions, and if those aren’t available, try en-GB then a.en. The main difference is in the formatting, and I think if they are user- or auto-generated.\n\n2\n\nThe XML data is one giant string. Split into a list using the &lt;p tag, which results in one string per caption item (timestamps, text, and various other tags).\n\n3\n\nEach caption item has a start time preceded by t=. This regex gets t=\"[numbers]\" and puts each result into a column called Start.\n\n4\n\nRemove t=\" from the beginning of the Start column and \" from the end, leaving a number that can be converted from a string to an integer.\n\n5\n\nThe XML data is one giant string. Split into a list using the &lt;p tag, which results in one string per caption item (timestamps, text, and various other tags).\n\n6\n\nThis is a list of the other variables that come with the caption text, including t= for start time and d= for duration. The formatting varies somewhat based on which version of captions you get, and the reason this selects the en captions before the en-GB and a.en captions is that this formatting is simpler.\n\n7\n\nTake all the tags/variables out, leaving only the actual caption text.\n\n8\n\nRemove special characters and extra spaces.\n\n9\n\nReturn as dataframe, with Text specified as a string column. If you leave it as the default object type, pandas will throw warnings later.\n\n10\n\nGet dataframes with caption times and caption texts, using two functions just defined. Join the results into one dataframe (this works because both are indexed by row number).\n\n11\n\nRemove rows where caption text is blank, then reset index (row number).\n\n12\n\nMake a column for the End time, which is the Start time of the new row - 1.\n\n\n\n\nRunning this takes a minute or so.\n\n1captions = get_captions(video_list)\n\n2captions = captions[captions['Episode'] &lt;= 84]\n\n3pd.concat([captions.head(), captions.tail()]).style.format(precision=0)\n\n\n1\n\nLoad and format captions for each video in the “all episodes” playlist, using functions defined above.\n\n2\n\nOnly analyze episodes 1-84.\n\n3\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \nEpisode\nText\nStart\nEnd\n\n\n\n\n0\n1\n[introductory music playing]\n160\n3189\n\n\n1\n1\nL: Welcome to Lingthusiasm a podcast that's\n3190\n6759\n\n\n2\n1\nenthusiastic about linguistics\n6760\n9218\n\n\n3\n1\nI'm Lauren Gawne.\n9219\n11319\n\n\n4\n1\nG: and I'm Gretchen McCulloch and today we're going to be talking about\n11320\n12399\n\n\n449\n84\nyour life who's curious about language. L: Lingthusiasm is created and produced by\n2298000\n2301899\n\n\n450\n84\nGretchen McCulloch and Lauren Gawne. Our Senior Producer is Claire Gawne,\n2301900\n2305559\n\n\n451\n84\nour Editorial Producer is Sarah Dopierala, our Production Assistant is Martha Tsutsui-Billins,\n2305560\n2311679\n\n\n452\n84\nand our Editorial Assistant is Jon Kruk. Our music is 'Ancient City' by The Triangles.\n2311680\n2316359\n\n\n453\n84\nG: Stay lingthusiastic! [Music]\n2316360\nnan\n\n\n\n\n\nSave results to data/captions.csv:\n\ncaptions.to_csv('data/captions.csv', index=False)\n\n\n\n1.5 Match Timestamps to Transcripts\nNow match the text from the transcript (target word start -/+ 25 characters) to the caption timestamps, using the thefuzz package to find the closest match.\n\ndef match_transcript_times(df_trans, df_cap):\n    \"\"\"Use fuzzy matching to match transcript turn to caption timestamp.\"\"\"\n\n1    df_times = df_trans.reset_index(drop=False)\n    to_cat = ['Vowel', 'Word', 'Episode']\n    df_times[to_cat] = df_times[to_cat].astype('category')\n2    df_times['Text_Caption'] = pd.Series(dtype='string')\n\n    for i in df_times.index:\n3        episode = df_times.loc[i, 'Episode']\n        captions_subset = df_cap[df_cap['Episode'] == episode]\n4        text = df_times.loc[i, 'Text_Subset']\n5        text = match_exceptions(text, search_exceptions)\n6        if text is not None:\n            match = process.extractOne(text, captions_subset['Text'])\n7            if match is not None:\n                df_times.loc[i, 'Text_Caption'] = match[0]\n                df_times.loc[i, 'Match_Rate'] = match[1]\n8                df_times.loc[i, 'Start'] = captions_subset.loc[match[2], 'Start']\n9                df_times.loc[i, 'End'] = captions_subset.loc[match[2], 'End']\n\n    return df_times\n\n\n1\n\nMake copy of the transcripts-containing-target-words dataframe, and convert Vowel, Word, and Episode from indices to columns, and specify those columns are categorical.\n\n2\n\nMake column for results and specify it is a string type. Again, this will work without this line, but will result in warnings from pandas.\n\n3\n\nSubset the captions dataframe to only include the episode of the current row, so we have less to search through.\n\n4\n\nText from transcript (target word -/+ 25 characters) to search for in captions.\n\n5\n\nThere are a few cases where the search text needs to be tweaked or subset to get the correct match, and a few cases where a token needs to be skipped because the audio quality wasn’t good enough. These cases are defined in the next code chunk.\n\n6\n\nFind row in caption dataframe (subset to just include the current episode) that best matches the search text (target word -/+ 25 characters).\n\n7\n\nIf a match is identified, a tuple is returned where the first item is the matching text, the second item is the certainty (where 100 is an exact match), and the third item is the row index (in the Text column of the captions dataframe, subset to just include the current episode).\n\n8\n\nUse the row number from the match to get the corresponding Start and End times.\n\n9\n\nResults include all of the target words found in the transcript, with columns added for Text_Caption, Start, and End.\n\n\n\n\nFuzzy string matching accounts for a lot of the differences between the transcripts (which are proofread/edited) and the captions (not all proofread/edited, have some speaker names and initials mixed in). But it wasn’t always able to account for the differences in splitting sections, since the captions aren’t split by speaker turns. So, there are some where matching the right timestamp requires a transcript text that’s subset further (search_text_subsets) or tweaked (search_text_replacements). This isn’t the most elegant solution, but it works (and it was still faster than opening up 84 30-minute audio files to find and extract out 200 3-second clips).\nThen, after going through all the clips and annotating the vowels (see Part 2), there are some that get skipped (search_text_skips) because of the audio quality (e.g., overlapping speech).\n\nsearch_exceptions = {\n1    'replace': {\n        \"all to a sing\": \"all to a single place\",\n        \"and you have a whole room full\":\n            \"speakers who have a way of communicating with each other that\",\n        \"'bang' or something\": \"bang or something\",\n        \"emphasising the beat gestures in a very\": \"B gestures\",\n        \"by the foot and they really want\":\n            \"Yeah they rented the books by the foot and they really want\",\n        \"father's brother's wife\": \"father's brother's wife\",\n        \"foot/feet\": \"foot feet\",\n        \"is known as a\": \"repetition is known as a\",\n        \"an elastic thing with a ball on the end.\":\n        \"paddle thing that has an elastic thing with a ball on the end\",\n        \"pin things\": \"trying to use language names as a way to pin things\",\n        \"red, green, yellow, blue, brown, pink, purple\":\n            \"L: Black, white, red, green, yellow, blue, brown\",\n        \"see; red, green, yellow, blue, brown, purple, pink\":\n            \"green, yellow, blue, brown, purple, pink,\",\n        \"tied up with\": \"complicated or really difficult histories\",\n        \"to cite\": \"believe I got to\",\n        \"your father's brother, and you\": \"your father's brother\",\n        \"your father's brothers or your\":\n            \"your father's brothers or your mother's brothers\"\n    },\n2    'trim': [\n        'a lot of very core', 'a more honorific one if you',\n        'an agreement among', 'audiobook', 'auditorily', 'ball of rocks',\n        'between those little balls', 'big yellow', 'bonus topics',\n        'bought a cot', 'bought! And it was true', 'bought some cheese',\n        'but let me finish', 'core idea', 'core metaphor',\n        'core set of imperatives', 'core thing', 'finished with their turn'\n        'finished yours', 'guest, Gabrielle', 'guest, Pedro',\n        'honorific register', 'how could this plan',\n        'imperative or this honorific', 'letter from her father',\n        'like if I bought', 'on which country you bought', 'pangrams', \n        'pater -- father', 'returning special guest',\n        'see among languages are based', 'substitute',\n        'that did it with the Big Bang', 'the idea is that honorifics',\n        'wanna hear more from our guests', 'with your elbow or your foot',\n        'You bet', 'you tap your foot by the treat'\n    ],\n3    'skip': [\n        'believe you left this', 'bit of a rise', 'By the foot!',\n        'can pin down', 'existing thanks', 'featuring', 'fun and delightful',\n        'fun to flip', 'fun with adults', 'group of things', 'his data?',\n        'in November', 'local differences', 'people notice', 'She says no',\n        'situation where', 'things wrong', 'what letters', 'who are around you'\n    ]\n}\n\ndef match_exceptions(text, exceptions):\n    \"\"\"Check if text to match is an exception and gets replaced or subset.\"\"\"\n\n4    for k, v in exceptions['replace'].items():\n        if k in text:\n            return v\n\n5    for s in exceptions['trim']:\n        if s in text:\n            return s\n\n6    return None if any(text in s for s in exceptions['skip']) else text\n\n\n1\n\nCases where search text needs to be replaced to match correctly.\n\n2\n\nCases where search text needs to be subset to match correctly.\n\n3\n\nCases where search text needs to be skipped because of audio quality.\n\n4\n\nIf a replace key string is a substring of search text, then return key’s value.\n\n5\n\nIf a trim string is a subset of search text, then return that string.\n\n6\n\nIf a skip string is a subset of search text, then return None. If none of exceptions apply, then return original search text.\n\n\n\n\nThis takes a few minutes to run.\n\n1timestamps = match_transcript_times(transcripts_subset, captions)\n2timestamps = timestamps[timestamps['Text_Caption'].notna()]\n3timestamps.to_csv('data/timestamps_all.csv', index=False)\n\n\n1\n\nGet timestamps for selected turns (function defined in two code chunks above).\n\n2\n\nMost ID a match, filter rows that don’t.\n\n3\n\nSave all results.\n\n\n\n\nNow select a subset to annotate. Pick the 5 highest matches (to caption timestamps from transcript) for each word + speaker, and if there are multiple good matches, pick the ones from the later episodes since those have higher sound quality.\n\ntimestamps = timestamps \\\n    .sort_values(['Match_Rate', 'Episode'], ascending=False) \\\n    .groupby(['Vowel', 'Word', 'Speaker'], observed=True) \\\n    .head(5) \\\n    .sort_values(['Vowel', 'Word', 'Speaker']) \\\n1    .reset_index(drop=True)\n\ntimestamps['Number'] = timestamps \\\n    .groupby(['Word', 'Speaker'], observed=True) \\\n2    .cumcount() + 1\n\ntimestamps = timestamps[[\n    'Vowel', 'Word', 'Speaker', 'Number', 'Episode', 'Turn',\n    'Text', 'Text_Subset', 'Text_Caption', 'Start', 'End'\n3]]\n\n\n1\n\nSort with the highest match rates and later episodes at the top. Then group by Vowel, Word, and Speaker. Select the 5 highest within each Vowel + Word + Speaker group, which gets the closest matches, and if there are multiple good matches, picks the one from the later episode (higher audio quality). Then re-sort by Vowel, Word, and Speaker and convert those from indices to columns.\n\n2\n\nAdd new column for count within each Word + Speaker combination, which we’ll use later to name files.\n\n3\n\nSort columns.\n\n\n\n\nWe have 5 tokens of most words (except for the words where there weren’t &gt;= 5 tokens in the transcripts):\n\ntimestamps.value_counts(['Vowel', 'Word', 'Speaker'], sort=False) \\\n    .reset_index() \\\n    .style \\\n1    .hide()\n\n\n1\n\nCount number of rows for each Vowel + Word + Speaker combination. Convert those from indices to columns, so this is a dataframe instead of a series, and thus can be printed as a table by making it a style object. .hide() drops the row numbers.\n\n\n\n\n\n\n\n\n\nVowel\nWord\nSpeaker\ncount\n\n\n\n\ni\nbeat\nGretchen\n5\n\n\ni\nbeat\nLauren\n5\n\n\ni\nbelieve\nGretchen\n5\n\n\ni\nbelieve\nLauren\n5\n\n\ni\npeople\nGretchen\n5\n\n\ni\npeople\nLauren\n5\n\n\nu\nblue\nGretchen\n5\n\n\nu\nblue\nLauren\n5\n\n\nu\nthrough\nGretchen\n5\n\n\nu\nthrough\nLauren\n5\n\n\nu\nwho\nGretchen\n5\n\n\nu\nwho\nLauren\n5\n\n\næ\nbang\nGretchen\n4\n\n\næ\nbang\nLauren\n2\n\n\næ\nhand\nGretchen\n5\n\n\næ\nhand\nLauren\n5\n\n\næ\nlaugh\nGretchen\n5\n\n\næ\nlaugh\nLauren\n2\n\n\nɑ\nball\nGretchen\n5\n\n\nɑ\nball\nLauren\n5\n\n\nɑ\nfather\nGretchen\n5\n\n\nɑ\nfather\nLauren\n5\n\n\nɑ\nhonorific\nGretchen\n4\n\n\nɑ\nhonorific\nLauren\n5\n\n\nɔ\nbought\nGretchen\n5\n\n\nɔ\nbought\nLauren\n4\n\n\nɔ\ncore\nGretchen\n5\n\n\nɔ\ncore\nLauren\n2\n\n\nɔ\nwrong\nGretchen\n5\n\n\nɔ\nwrong\nLauren\n5\n\n\nə\namong\nGretchen\n5\n\n\nə\namong\nLauren\n5\n\n\nə\nfamous\nGretchen\n5\n\n\nə\nfamous\nLauren\n5\n\n\nə\nsupport\nGretchen\n5\n\n\nə\nsupport\nLauren\n5\n\n\nɛ\nbet\nGretchen\n5\n\n\nɛ\nbet\nLauren\n5\n\n\nɛ\nguest\nGretchen\n5\n\n\nɛ\nguest\nLauren\n3\n\n\nɛ\nsays\nGretchen\n5\n\n\nɛ\nsays\nLauren\n5\n\n\nɪ\nbit\nGretchen\n5\n\n\nɪ\nbit\nLauren\n5\n\n\nɪ\nfinish\nGretchen\n5\n\n\nɪ\nfinish\nLauren\n5\n\n\nɪ\npin\nGretchen\n5\n\n\nɪ\npin\nLauren\n5\n\n\nʊ\ncould\nGretchen\n5\n\n\nʊ\ncould\nLauren\n5\n\n\nʊ\nfoot\nGretchen\n5\n\n\nʊ\nfoot\nLauren\n5\n\n\nʊ\nput\nGretchen\n5\n\n\nʊ\nput\nLauren\n5\n\n\nʌ\nanother\nGretchen\n5\n\n\nʌ\nanother\nLauren\n5\n\n\nʌ\nbut\nGretchen\n5\n\n\nʌ\nbut\nLauren\n5\n\n\nʌ\nfun\nGretchen\n5\n\n\nʌ\nfun\nLauren\n5\n\n\n\n\n\nSave results to data/timestamps.csv:\n\ntimestamps.to_csv('data/timestamps_annotate.csv', index=False)\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2024,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2024-02-07},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2024. “Lingthusiasm Vowel Plots.”\nFebruary 7, 2024. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "3_plot_vowels.html",
    "href": "3_plot_vowels.html",
    "title": "Part 3: Plotting the Vowels",
    "section": "",
    "text": "Now to actually make the vowel plots! This document goes into detail about how I decided to make them the way I did and how to implement them in ggplot, but if you just want to see the final results, jump down to here, here, and here.\n\n3.1 Setup\n\n1library(tidyverse)\n2library(magrittr)\n3library(ggtext)\n4library(ggforce)\n5library(ggrepel)\n6library(rcartocolor)\n7library(png)\n8library(patchwork)\n\n9options(dplyr.summarise.inform = FALSE)\n\n\n1\n\nData wrangling (tidyr, dplyr, purrr, stringr), ggplot2 for plotting.\n\n2\n\nPipe operator.\n\n3\n\nMarkdown/HTML formatting for text in plots.\n\n4\n\nEllipsis plots.\n\n5\n\nOffset text labels from points.\n\n6\n\nColor themes.\n\n7\n\nOpen PNG images.\n\n8\n\nAdd images on top of plots.\n\n9\n\nDon’t print a message every time summarise() is called on a grouped dataframe.\n\n\n\n\n(Note: this could be done in Python, but I strongly prefer the ggplot package for plotting.)\n\nData\nLoad the vowel formant data from Part 2:\n\n1formants &lt;- read.csv(\"data/formants.csv\", stringsAsFactors = TRUE) %&gt;%\n  select(-Vowel_Time, -Count) %&gt;%\n2  mutate(\n    Speaker = ifelse(Speaker == \"G\", \"Gretchen\", \"Lauren\"),\n    List = ifelse(\n      List == \"episode\", \"Lingthusiasm Episodes\", \"Wells Lexical Set\"\n    )\n  ) %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nstr(formants)\n\n\n1\n\nRead formants data from 2_annotate_audio.qmd, and keep columns for List, Vowel, Word, Speaker, F1, and F2.\n\n2\n\nRecode the values for Speaker and List from abbreviations to full strings for plot labels, then make them both factors.\n\n\n\n\n'data.frame':   397 obs. of  6 variables:\n $ List   : Factor w/ 2 levels \"Lingthusiasm Episodes\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Vowel  : Factor w/ 11 levels \"ɑ\",\"æ\",\"ɔ\",\"ə\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ Word   : Factor w/ 47 levels \"among\",\"another\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Speaker: Factor w/ 2 levels \"Gretchen\",\"Lauren\": 1 1 1 1 1 2 2 2 2 2 ...\n $ F1     : num  766 602 733 766 626 ...\n $ F2     : num  1260 1254 1175 1373 1260 ...\n\n\n\n\nIPA Symbols\nHowever, the IPA symbols aren’t encoded correctly. They’ll render in RStudio, but not when Quarto renders the document to HTML, or always when ggplot renders the plots. This isn’t what we want:\n\nɑ, æ, ɔ, ə, ɛ, i, ɪ, o, u, ʊ, ʌ\n\nSo, the next step is to enter the unicode values manually (copied from this Wikipedia page):\n\nvowels &lt;- c(\n  \"i_lower\"   = \"\\u0069\",  # i (close front unrounded)\n  \"i_upper\"   = \"\\u026A\",  # ɪ (near-close front unrounded)\n  \"epsilon\"   = \"\\u025B\",  # ɛ (open-mid front unrounded)\n  \"ash\"       = \"\\u00E6\",  # æ (near-open front unrounded)\n  \"schwa\"     = \"\\u0259\",  # ə (mid central)\n  \"horseshoe\" = \"\\u028A\",  # ʊ (near-close near-back rounded)\n  \"u\"         = \"\\u0075\",  # u (close back rounded)\n  \"o\"         = \"\\u006F\",  # o (close-mid back rounded)\n  \"hat\"       = \"\\u028C\",  # ʌ (open-mid back unrounded)\n  \"open_o\"    = \"\\u0254\",  # ɔ (open-mid back rounded)\n  \"alpha\"     = \"\\u0251\"   # ɑ (open back unrounded)\n)\n\nThese are ordered from front to back, then close to open (Figure 4).\nThen match the unicode for the IPA symbol to the words:\n\nformants %&lt;&gt;% mutate(\n1  Vowel = case_when(\n    Word %in% c(\"ball\", \"father\", \"honorific\", \"lot\", \"palm\", \"start\") ~ vowels[\"alpha\"],\n    Word %in% c(\"bang\", \"bath\", \"hand\", \"laugh\", \"trap\") ~ vowels[\"ash\"],\n    Word %in% c(\"bought\", \"cloth\", \"core\", \"north\", \"thought\", \"wrong\") ~ vowels[\"open_o\"],\n    Word %in% c(\"among\", \"famous\", \"support\") ~ vowels[\"schwa\"],\n    Word %in% c(\"bet\", \"dress\", \"guest\", \"says\", \"square\") ~ vowels[\"epsilon\"],\n    Word %in% c(\"beat\", \"believe\", \"fleece\", \"people\") ~ vowels[\"i_lower\"],\n    Word %in% c(\"bit\", \"finish\", \"kit\", \"near\", \"pin\") ~ vowels[\"i_upper\"],\n    Word %in% c(\"force\", \"goat\") ~ vowels[\"o\"],\n    Word %in% c(\"blue\", \"goose\", \"through\", \"who\") ~ vowels[\"u\"],\n    Word %in% c(\"could\", \"cure\", \"put\", \"foot\") ~ vowels[\"horseshoe\"],\n    Word %in% c(\"another\", \"but\", \"fun\", \"strut\") ~ vowels[\"hat\"],\n2  ) %&gt;% factor(levels = vowels, ordered = TRUE)\n)\n\nstr(formants)\n\n\n1\n\nIf the value in the Word column is ball, father, honorific, lot, or palm, then assign the alpha value from the vowels list.\n\n2\n\nConvert character to factor, then specify the order of the factors (same as in vowels list above) to make sure it stays consistent.\n\n\n\n\n'data.frame':   397 obs. of  6 variables:\n $ List   : Factor w/ 2 levels \"Lingthusiasm Episodes\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Vowel  : Ord.factor w/ 11 levels \"i\"&lt;\"ɪ\"&lt;\"ɛ\"&lt;\"æ\"&lt;..: 5 5 5 5 5 5 5 5 5 5 ...\n  ..- attr(*, \"names\")= chr [1:397] \"schwa\" \"schwa\" \"schwa\" \"schwa\" ...\n $ Word   : Factor w/ 47 levels \"among\",\"another\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Speaker: Factor w/ 2 levels \"Gretchen\",\"Lauren\": 1 1 1 1 1 2 2 2 2 2 ...\n $ F1     : num  766 602 733 766 626 ...\n $ F2     : num  1260 1254 1175 1373 1260 ...\n\n\nNow the IPA vowels consistently render correctly:\n\ni, ɪ, ɛ, æ, ə, ʊ, u, o, ʌ, ɔ, ɑ\n\n\n\nLingthusiasm Theme\nThe Lingthusiasm font is Josefin Sans, which is available from Google Fonts.\nI downloaded and installed it to my computer. There are a number of different ways to add new fonts without having to install them separately outside of RStudio, such as font_add_google() from the showtext package. However, that method was causing errors rendering the IPA symbols.\nsystemfonts() shows the list of fonts installed on my computer that R recognizes, and it finds Josefin Sans:\n\n1systemfonts::system_fonts() %&gt;%\n2  filter(str_detect(family, \"Josefin Sans\")) %&gt;%\n3  select(path, name, family) %&gt;%\n  pivot_longer(cols = everything())\n\n\n1\n\nGet dataframe of fonts available.\n\n2\n\nFilter to include Josefin Sans.\n\n3\n\nSelect columns to print; flip to list vertically.\n\n\n\n\n# A tibble: 3 × 2\n  name   value                                                                  \n  &lt;chr&gt;  &lt;chr&gt;                                                                  \n1 path   \"C:\\\\Users\\\\betha\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\JosefinS…\n2 name   \"JosefinSans-Thin\"                                                     \n3 family \"Josefin Sans\"                                                         \n\n\nHowever, the fonts loaded by default just include Times New Roman, Arial, and Courier New:\n\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\n\nThis tells R to load Josefin Sans into the set of available fonts, so text will render in Josefin Sans if family = sans_alt, but stick with the default sans font otherwise (and not break the IPA symbols).\n\nwindowsFonts(sans_alt = \"Josefin Sans\")\nwindowsFonts()\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\n$sans_alt\n[1] \"Josefin Sans\"\n\n\nThe hex codes for the green and navy are:\n\nlingthusiasm_green = \"#26b14c\"\nlingthusiasm_navy = \"#051458\"\n\nAnd the logo:\n\n1lingthusiasm_logo &lt;- readPNG(\"resources/lingthusiasm_logo_circle.png\", native = TRUE)\nlingthusiasm_tagline &lt;- readPNG(\"resources/lingthusiasm_logo_tagline.png\", native = TRUE)\n\n\n1\n\nRead logo images. native = TRUE specifies reading it as a raster object instead of an array, which is the format patchwork::inset_element() needs.\n\n\n\n\nPutting it together:\n\ntibble(\n1  \"Color\" = c(\"green\", \"navy\"),\n2  \"Hex\" = c(lingthusiasm_green, lingthusiasm_navy),\n3  \"Extra_Col\" = c(1, 1)\n) %&gt;%\n4  ggplot(aes(x = Color, y = Extra_Col, fill = Hex, label = Hex)) +\n5  geom_tile() +\n6  geom_text(size = 10, color = \"white\") +\n7  scale_fill_identity() +\n  theme_classic() +\n8  labs(title = \"Lingthusiasm Theme\") +\n  theme(\n    plot.title = element_text(\n      family = \"sans_alt\", size = 28,\n      margin = margin(t = 1, b = 1, unit = \"lines\"), hjust = 0.55,\n      color = lingthusiasm_navy\n    ),  \n9    axis.text = element_blank(), axis.title = element_blank(),\n    axis.line = element_blank(), axis.ticks = element_blank()\n  ) +\n10  inset_element(\n    p = lingthusiasm_logo,\n11    left = unit(0.05, \"snpc\"), right = unit(0.25, \"snpc\"),\n    top = unit(1.2, \"snpc\"), bottom = unit(1, \"snpc\")\n  )\n\n\n1\n\nColor is names of the two lingthusiasm theme colors.\n\n2\n\nHex is the two hex codes.\n\n3\n\nExtra_Col is a dummy value because ggplot needs a Y axis.\n\n4\n\nThe X axis is Color, and the Y axis is Extra_Col, which just creates two boxes next to each other. Fill and label are specified by Hex.\n\n5\n\nDraw a square for each color.\n\n6\n\nLabel the squares with the hex code strings (keeping the default font).\n\n7\n\nFill the squares with the hex code color values.\n\n8\n\nSet the plot title text to be navy, Josefin Sans, size 28, centered with some space above and below.\n\n9\n\nRemove the axis lines, labels, titles, and ticks.\n\n10\n\nUse the patchwork package to add the logo image on top of the plot. This step needs to be last.\n\n11\n\nSpecify the positions for each corner of the logo, using spnc units so it stays square even if the overall plot is rectangular.\n\n\n\n\n\n\n\nFigure 1: Lingthusiasm colors, font, and logo.\n\n\n\n\n\n\n\n3.2 Plot Vowel Means\nNow let’s take a look at the data! F1 gets plotted on the Y axis, and F2 gets plotted on the X axis.\n\n1means_1 &lt;- formants %&gt;%\n  group_by(Speaker, List, Vowel) %&gt;%\n  summarise(F1 = mean(F1), F2 = mean(F2)) %&gt;%\n2  ggplot(aes(x = F2, y = F1, label = Vowel)) +\n3  geom_textbox(\n4    fill = lingthusiasm_green, box.colour = NA,\n5    color = \"white\", size = 4.5, halign = 0.5, valign = 0.5,\n6    width = unit(0.10, \"snpc\"), height = unit(0.10, \"snpc\"),\n7    box.padding = unit(c(0, 0, 0, 0), \"snpc\"), box.r = unit(0.01, \"snpc\")\n  ) +  \n8  facet_grid(Speaker ~ List) +\n9  theme_classic() +\n  theme(\n10    axis.line = element_line(color = lingthusiasm_navy),\n    axis.ticks = element_line(color = lingthusiasm_navy),\n    panel.border = element_rect(color = lingthusiasm_navy, fill = NA),\n    strip.background = element_rect(color = lingthusiasm_navy),\n11    text = element_text(size = 12, family = \"sans_alt\", color = lingthusiasm_navy),\n    axis.text = element_text(color = lingthusiasm_navy),\n    strip.text = element_text(color = lingthusiasm_navy, size = 12)\n  ) +\n12  labs(title = \"Vowel Means\")\n\nmeans_1\n\n\n1\n\nTake the full data set, group it by Speaker then List then Vowel, and then calculate the means of F1 and F2 for each Speaker x List x Vowel.\n\n2\n\nAll layers of the plot have F2 on the X axis, F1 on the Y axis, and are labelled by Vowel.\n\n3\n\nWrite the vowel symbols (because Label = Vowel) at the location of their means.\n\n4\n\nMake the text box background lingthusiasm green with no outline.\n\n5\n\nMake the text white, size 4.5 (note that this is on a different scale than the rest of the text sizes specified in later theme()), vertically and horizontally centered.\n\n6\n\nSet the size of the text boxes, using snpc (squared normalized parent coordinates) to be relative to the size of the plot but always square.\n\n7\n\nNo margins inside the text boxes and a slight curve on the corners.\n\n8\n\nSplit the plot to have Gretchen’s data in the top panels and Lauren’s data in the bottom panels, and the data from the Lingthusiasm episodes in the left panels and the data from the Wells lexical set recordings in the right panels.\n\n9\n\nChange the default theme to have a white background with no grid lines.\n\n10\n\nChange all the lines (axis lines, axis ticks, outline around panels, outline around panel labels) to be the lingthusiasm navy.\n\n11\n\nMake all the text navy Josefin Sans. Set the base size as 12, but make the text of the speaker panel labels bigger.\n\n12\n\nSet the title, and leave the other axis/legend labels as their default values of “F1”, “F2”, and “Vowel.”\n\n\n\n\n\n\n\nFigure 2: Vowel means (default axes).\n\n\n\n\n(Sidenote: saving the theme specifications so we don’t have to keep retyping theme.)\n\nlingthusiasm_theme &lt;- theme(\n  axis.line = element_line(color = lingthusiasm_navy),\n  axis.ticks = element_line(color = lingthusiasm_navy),\n  panel.border = element_rect(color = lingthusiasm_navy, fill = NA),\n  strip.background = element_rect(color = lingthusiasm_navy),\n  text = element_text(size = 12, family = \"sans_alt\", color = lingthusiasm_navy),\n  axis.text = element_text(color = lingthusiasm_navy),\n  strip.text = element_text(color = lingthusiasm_navy, size = 12)\n) \n\nHowever, vowel plots typically have their axes reversed, so that the highest value of (F1, F2) is at the bottom left corner instead of the top right corner. This isn’t standard data visualization procedure, but it has a cool and useful result.\n\n1means_2 &lt;- formants %&gt;%\n  group_by(List, Speaker, Vowel) %&gt;%\n  summarise(F1 = mean(F1), F2 = mean(F2)) %&gt;%\n  ggplot(aes(x = F2, y = F1, label = Vowel)) +\n  geom_textbox(\n    fill = lingthusiasm_green, box.colour = NA,\n    color = \"white\", size = 4.5, halign = 0.5, valign = 0.5,\n    width = unit(0.10, \"snpc\"), height = unit(0.10, \"snpc\"),\n    box.padding = unit(c(0, 0, 0, 0), \"snpc\"), box.r = unit(0.01, \"snpc\")\n  ) +  \n  facet_grid(Speaker ~ List) +\n2  scale_x_reverse(breaks = c(1000, 1500, 2000, 2500)) +\n3  scale_y_reverse(limits = c(1050, 225), n.breaks = 4) +\n  theme_classic() +\n4  lingthusiasm_theme +\n  labs(title = \"Vowel Means\")\n\nmeans_2\n\n\n1\n\nJust annotating the lines that changed from the previous chunk.\n\n2\n\nAdd this to flip the X axis. Specify breaks because the default values aren’t even.\n\n3\n\nAdd this to flip the Y axis. The limits (see how they’re reversed) are specified because the defaults were a bit too narrow, and like this the axis ticks/labels are spaced more evenly.\n\n4\n\nAdd this to specify the colors and font sizes etc.\n\n\n\n\n\n\n\nFigure 3: Vowel means (reversed axes).\n\n\n\n\nNow the layout resembles the IPA vowel chart! Front vowels are on the left, and back vowels are on the right; close vowels are on the top, and open vowels are on the bottom.\n\n\n\nFigure 4: IPA Vowel Chart.\n\n\n\n\n3.3 Plot Individual Data Points\nJust plotting the means for each vowel loses a lot of information, so let’s take a look at the underlying data.\nNow, we’ll distinguish between vowels by color. First, make a legend that will be easier to read than the default by creating a string that prints each vowel in its corresponding color (using the ggtext package to render the HTML formatting).\n\n1vowel_key &lt;- tibble(\"Vowel\" = vowels, \"Color\" = carto_pal(12, \"Bold\")[1:11]) %&gt;%\n2  mutate(Styled = str_c(\"&lt;b style='color:\", Color, \"'&gt;\", Vowel, \"&lt;/b&gt;\")) %&gt;%\n3  pull(Styled) %&gt;%\n  str_flatten(collapse = \", \")\n\nvowel_key %&gt;% str_wrap(32) %&gt;% str_view()\n\n\n1\n\nVowel column is the list of vowels (unicode codes). Color column is the hex codes from the Bold palette in rcartocolor, the color set we’ve been using so far. (Using the first 11 values from the full palette, so the last color isn’t gray.)\n\n2\n\nEncase with HTML code, so that hex code becomes a color argument for the vowel character.\n\n3\n\nMerge into 1 string, with each value separated by a comma + space. 4.. Print, wrapping lines on each item.\n\n\n\n\n[1] │ &lt;b style='color:#7F3C8D'&gt;i&lt;/b&gt;,\n    │ &lt;b style='color:#11A579'&gt;ɪ&lt;/b&gt;,\n    │ &lt;b style='color:#3969AC'&gt;ɛ&lt;/b&gt;,\n    │ &lt;b style='color:#F2B701'&gt;æ&lt;/b&gt;,\n    │ &lt;b style='color:#E73F74'&gt;ə&lt;/b&gt;,\n    │ &lt;b style='color:#80BA5A'&gt;ʊ&lt;/b&gt;,\n    │ &lt;b style='color:#E68310'&gt;u&lt;/b&gt;,\n    │ &lt;b style='color:#008695'&gt;o&lt;/b&gt;,\n    │ &lt;b style='color:#CF1C90'&gt;ʌ&lt;/b&gt;,\n    │ &lt;b style='color:#F97B72'&gt;ɔ&lt;/b&gt;,\n    │ &lt;b style='color:#4B4B8F'&gt;ɑ&lt;/b&gt;\n\n\nWhich will render like this:\n\ni, ɪ, ɛ, æ, ə, ʊ, u, o, ʌ, ɔ, ɑ\n\n(Note that ggplot will throw a warning like Warning in text_info(label, fontkey, fontfamily, font, fontsize, cache): unable to translate '&lt;U+0251&gt;png215' to native encoding, but it renders correctly, so the warnings are turned off in those code chunks.)\n\n1points &lt;- formants %&gt;%\n  ggplot(aes(x = F2, y = F1, color = Vowel, label = Vowel)) +\n2  geom_point(size = 1.5) +\n  facet_grid(Speaker ~ List) +\n3  scale_color_manual(values = carto_pal(12, \"Bold\")) +\n4  scale_x_reverse(breaks = c(750, 1250, 1750, 2250, 2750)) +\n  scale_y_reverse(breaks = c(250, 500, 750, 1000)) +\n  theme_classic() +\n  lingthusiasm_theme +\n5  theme(plot.subtitle = element_markdown(family = \"sans\")) +\n6  labs(title = \"Individual Data Points\", subtitle = vowel_key) +\n7  guides(color = guide_none())\n\npoints\n\n\n1\n\nPassing the full data set, not the means by Speaker + Vowel + List, to ggplot.\n\n2\n\nInstead of geom_text(), geom_point() is a layer drawing scatterplot (size making the points slightly bigger than default).\n\n3\n\nUse the Bold color palette from the rcartocolor package to color-code the vowels. (There are 11 vowels, but I specify 12 colors here so the grey gets skipped.)\n\n4\n\nLimits need to be slightly bigger than plots with vowel means, and then breaks adjusted so that that Y axis labels don’t overlap with each other between the two panels.\n\n5\n\nelement_markdown() from ggtext will render the HTML string. Use default sans serif font because Josefin Sans doesn’t have all of the IPA symbols.\n\n6\n\nAdd the color-coded list of vowels as a subtitle.\n\n7\n\nTurn the default legend off.\n\n\n\n\n\n\n\nFigure 5: Individual data points.\n\n\n\n\n\n\n3.4 Plot Word Means\nThe data for each vowel consists of 3 different words. How different are they? First, let’s look at the Wells Lexical Set.\nThese next plots use the ggrepel package to make the word labels not overlap with each other or with the scatterplot points.\n\n1words_ls &lt;- formants %&gt;%\n  filter(List == \"Wells Lexical Set\") %&gt;%\n2  group_by(Speaker, Vowel, Word) %&gt;%\n  summarise(F1 = mean(F1), F2 = mean(F2)) %&gt;%\n3  ggplot(aes(x = F2, y = F1, color = Vowel, label = Word)) +\n4  geom_point(size = 1.25) +\n5  geom_label_repel(\n6    min.segment.length = 0,\n7    size = 4,\n8    force = 75,\n9    family = \"sans_alt\",\n10    seed = 2024\n  ) +\n11  facet_wrap(~Speaker) +\n  scale_color_manual(values = carto_pal(12, \"Bold\")) +\n12  scale_x_reverse(breaks = c(750, 1250, 1750, 2250, 2750)) +\n  scale_y_reverse(breaks = c(250, 500, 750, 1000)) +\n  theme_classic() +\n  lingthusiasm_theme +\n  theme(plot.subtitle = element_markdown(size = 15, family = \"sans\")) +\n13  labs(title = \"Means By Word: Wells Lexical Set\", subtitle = vowel_key) +\n  guides(color = guide_none())\n\nwords_ls\n\n\n1\n\nOnly include Wells Lexical Set word list.\n\n2\n\nThe data for this plot is the means by Speaker, Vowel, AND Word.\n\n3\n\nAll layers of this plot have F2 on the X axis, F1 on the Y axis, are color-coded by Vowel, and are labelled by Vowel.\n\n4\n\nDraw a point at each Speaker*Vowel*Word mean (slightly bigger than default).\n\n5\n\nDraw text box labels offset from each point. geom_label_repel() makes sure none of the boxes overlap with the points or with each other.\n\n6\n\nAlways draw a line from the text box to the scatterplot point,\n\n7\n\nText size. Note this is on a different scale than the text for the title/axis labels.\n\n8\n\nIncrease the amount of space required between the text boxes.\n\n9\n\nMake font Josefin sans.\n\n10\n\nSet a seed so the results are consistent.\n\n11\n\nPut Gretchen on the left and Lauren on the right.\n\n12\n\nSpecify limits and locations of labels/breaks, since the defaults aren’t even.\n\n13\n\nInclude color-coded vowel string as a subtitle, instead of the default legend for color.\n\n\n\n\n\n\n\nFigure 6: Mean for each word in the Wells Lexical Set word list.\n\n\n\n\nNow let’s look at the Lingthusiasm episode words:\n\n1words_ep_1 &lt;- formants %&gt;%\n  filter(List == \"Lingthusiasm Episodes\") %&gt;%\n  group_by(Speaker, Vowel, Word) %&gt;%\n  summarise(F1 = mean(F1), F2 = mean(F2)) %&gt;%\n  ggplot(aes(x = F2, y = F1, color = Vowel, label = Word)) +\n  geom_point(size = 1.25) +\n  geom_label_repel(\n    min.segment.length = 0, force = 75, seed = 2024,\n    size = 4, family = \"sans_alt\"\n  ) +\n2  facet_wrap(~Speaker, ncol = 1) +\n  scale_color_manual(values = carto_pal(12, \"Bold\")) +\n  scale_x_reverse(breaks = c(750, 1250, 1750, 2250, 2750)) +\n  scale_y_reverse(breaks = c(250, 500, 750, 1000)) +\n  theme_classic() +\n  lingthusiasm_theme +\n  theme(plot.subtitle = element_markdown(size = 15, family = \"sans\")) +\n  labs(title = \"Means By Word: Lingthusiasm Episodes\", subtitle = vowel_key) +\n  guides(color = guide_none())\n\nwords_ep_1\n\n\n1\n\nOnly include Lingthusiasm Episode word list.\n\n2\n\nThe panels are stacked vertically, because the word labels take up more space. fig-asp: 1.25 in this code chunk’s header makes it render tall enough.\n\n\n\n\n\n\n\nFigure 7: Mean for each word in the Lingthusiasm Episode word list.\n\n\n\n\nOne thing that makes this plot a bit hard to interpret is that it’s not immediately clear which vowel in the word is the one being plotted. So, let’s make the vowel bold relative to the rest of the word.\nSo far we’ve been using ggtext to format text, but that doesn’t work with ggrepel. The workaround, like with the IPA vowels, is to just enter the unicode characters directly.\nThese are the codes for the Mathematical Sans Serif capital letters, in regular and bold faces. They’re copy-pasted in here manually even though the pattern is predictable, because procedurally generating strings with the \\u prefix is a pain.\n\nalphabet_reg &lt;- c(\n  \"\\U1D5A0\", \"\\U1D5A1\", \"\\U1D5A2\", \"\\U1D5A3\", \"\\U1D5A4\", \"\\U1D5A5\",\n  \"\\U1D5A6\", \"\\U1D5A7\", \"\\U1D5A8\", \"\\U1D5A9\", \"\\U1D5AA\", \"\\U1D5AB\",\n  \"\\U1D5AC\", \"\\U1D5AD\", \"\\U1D5AE\", \"\\U1D5AF\", \"\\U1D5B0\", \"\\U1D5B1\",\n  \"\\U1D5B2\", \"\\U1D5B3\", \"\\U1D5B4\", \"\\U1D5B5\", \"\\U1D5B6\", \"\\U1D5B7\",\n  \"\\U1D5B8\", \"\\U1D5B9\"\n)\n\nalphabet_bold &lt;- c(\n  \"\\U1D5D4\", \"\\U1D5D5\", \"\\U1D5D6\", \"\\U1D5D7\", \"\\U1D5D8\", \"\\U1D5D9\",\n  \"\\U1D5DA\", \"\\U1D5DB\", \"\\U1D5DC\", \"\\U1D5DD\", \"\\U1D5DE\", \"\\U1D5DF\",\n  \"\\U1D5E0\", \"\\U1D5E1\", \"\\U1D5E2\", \"\\U1D5E3\", \"\\U1D5E4\", \"\\U1D5E5\",\n  \"\\U1D5E6\", \"\\U1D5E7\", \"\\U1D5E8\", \"\\U1D5E9\", \"\\U1D5EA\", \"\\U1D5EB\",\n  \"\\U1D5EC\", \"\\U1D5ED\"\n)\n\n1names(alphabet_reg) &lt;- letters[1:26]\nnames(alphabet_bold) &lt;- letters[1:26]\n\n\n1\n\nName vectors with regular letters, to access similar to a python dictionary.\n\n\n\n\nFirst, convert the whole word to the regular letters:\n\n1to_unicode_caps &lt;- function(word, alphabet_reg) {\n2  letters &lt;- str_split(word, pattern = \"\")\n3  converted &lt;- \"\"\n4  for (l in letters) {\n    new_word &lt;- str_c(converted, alphabet_reg[l])\n  }\n5  return(str_flatten(new_word))\n}\n\n\n1\n\nFunction takes word as a string and alphabet_reg as a named list.\n\n2\n\nSplit the word into individual letters.\n\n3\n\nStart string for the converted word.\n\n4\n\nFor each letter, use the fact that alphabet_reg is named with the regular letters to get the unicode string for the current letter. Concatenate the letter pulled from alphabet_reg to the converted string.\n\n5\n\nCombine list of letters back into one string.\n\n\n\n\n\n1words_unicode &lt;- map(formants$Word, to_unicode_caps, alphabet_reg)\n\n2formants %&lt;&gt;% mutate(.after = Word, Word_Label = words_unicode) %&gt;%\n3  unnest(Word_Label)\n\n\n1\n\nFor each item in the Word column of the formants dataframe, call the function to_unicode_caps() (defined in previous code chunk) on it. Pass alphabet_reg as the second argument to to_unicode_caps().\n\n2\n\nInsert the words_unicode into the formants dataframe as a column called Word_Label, after the Word column.\n\n3\n\nConvert the items in Word_Label from lists containing 1 string to just strings.\n\n\n\n\nWhich renders as:\n\n𝖠𝖬𝖮𝖭𝖦, 𝖠𝖭𝖮𝖳𝖧𝖤𝖱, 𝖡𝖠𝖫𝖫, 𝖡𝖠𝖭𝖦, 𝖡𝖤𝖠𝖳, 𝖡𝖤𝖫𝖨𝖤𝖵𝖤, 𝖡𝖤𝖳, 𝖡𝖨𝖳, 𝖡𝖫𝖴𝖤, 𝖡𝖮𝖴𝖦𝖧𝖳, 𝖡𝖴𝖳, 𝖢𝖮𝖱𝖤, 𝖢𝖮𝖴𝖫𝖣, 𝖥𝖠𝖬𝖮𝖴𝖲, 𝖥𝖠𝖳𝖧𝖤𝖱, 𝖥𝖨𝖭𝖨𝖲𝖧, 𝖥𝖮𝖮𝖳, 𝖥𝖴𝖭, 𝖦𝖴𝖤𝖲𝖳, 𝖧𝖠𝖭𝖣, 𝖧𝖮𝖭𝖮𝖱𝖨𝖥𝖨𝖢, 𝖫𝖠𝖴𝖦𝖧, 𝖯𝖤𝖮𝖯𝖫𝖤, 𝖯𝖨𝖭, 𝖯𝖴𝖳, 𝖲𝖠𝖸𝖲, 𝖲𝖴𝖯𝖯𝖮𝖱𝖳, 𝖳𝖧𝖱𝖮𝖴𝖦𝖧, 𝖶𝖧𝖮, 𝖶𝖱𝖮𝖭𝖦, 𝖡𝖠𝖳𝖧, 𝖢𝖫𝖮𝖳𝖧, 𝖢𝖴𝖱𝖤, 𝖣𝖱𝖤𝖲𝖲, 𝖥𝖫𝖤𝖤𝖢𝖤, 𝖥𝖮𝖱𝖢𝖤, 𝖦𝖮𝖠𝖳, 𝖦𝖮𝖮𝖲𝖤, 𝖪𝖨𝖳, 𝖫𝖮𝖳, 𝖭𝖤𝖠𝖱, 𝖯𝖠𝖫𝖬, 𝖲𝖰𝖴𝖠𝖱𝖤, 𝖲𝖳𝖠𝖱𝖳, 𝖲𝖳𝖱𝖴𝖳, 𝖳𝖧𝖮𝖴𝖦𝖧𝖳, 𝖳𝖱𝖠𝖯\n\nThen convert the corresponding vowels to bold face.\n\n1formants %&lt;&gt;% mutate(\n2  Word_Label = ifelse(\n3    Word %in% c(\n      \"ball\", \"bang\", \"bath\", \"beat\", \"father\", \"famous\", \"goat\", \"hand\",\n      \"laugh\", \"near\", \"palm\", \"says\", \"square\", \"start\", \"trap\"\n    ),\n4    str_replace(Word_Label, alphabet_reg[\"a\"], alphabet_bold[\"a\"]),\n5    Word_Label\n  ),\n6  Word_Label = ifelse(\n    Word %in% c(\n      \"beat\", \"bet\", \"blue\", \"dress\", \"fleece\", \"guest\", \"near\", \"people\"\n    ),\n    str_replace(Word_Label, alphabet_reg[\"e\"], alphabet_bold[\"e\"]),\n    Word_Label\n  ),\n7  Word_Label = ifelse(\n    Word %in% c(\"bit\", \"finish\", \"kit\", \"pin\"),\n    str_replace(Word_Label, alphabet_reg[\"i\"], alphabet_bold[\"i\"]),\n    Word_Label\n  ),\n8  Word_Label = ifelse(\n    Word %in% c(\n      \"among\", \"another\", \"bought\", \"cloth\", \"core\", \"could\",\n      \"force\", \"goat\", \"honorific\", \"lot\", \"people\", \"thought\",\n      \"through\", \"who\", \"wrong\"\n    ),\n    str_replace(Word_Label, alphabet_reg[\"o\"], alphabet_bold[\"o\"]),\n    Word_Label\n  ),\n9  Word_Label = ifelse(\n    Word %in% c(\n      \"bought\", \"blue\", \"but\", \"could\", \"fun\", \"guest\", \"laugh\",\n      \"put\", \"strut\", \"square\", \"support\", \"thought\", \"through\"\n    ),\n    str_replace(Word_Label, alphabet_reg[\"u\"], alphabet_bold[\"u\"]),\n    Word_Label\n  ),\n10  Word_Label = ifelse(\n    Word == \"believe\",\n    str_replace(\n      Word_Label,\n      str_c(alphabet_reg[\"i\"], alphabet_reg[\"e\"]),\n      str_c(alphabet_bold[\"i\"], alphabet_bold[\"e\"])\n    ),\n    Word_Label\n  ),\n11  Word_Label = ifelse(\n    Word %in% c(\"goose\", \"foot\"),\n    str_replace_all(\n      Word_Label,\n      str_c(alphabet_reg[\"o\"], alphabet_reg[\"o\"]),\n      str_c(alphabet_bold[\"o\"], alphabet_bold[\"o\"])\n    ),\n    Word_Label\n  ),\n12  Word_Label = ifelse(\n    Word == \"fleece\",\n    str_replace(Word_Label, alphabet_reg[\"e\"], alphabet_bold[\"e\"]),\n    Word_Label\n  )\n)\n13formants$Word_Label %&lt;&gt;% as.factor()\n\n\n1\n\nMutating the Word_Label column multiple times, because several words have multiple vowels to swap. Swapping one vowel at a time is shorter than swapping one category of word at time.\n\n2\n\nFirst modification to Word_Label is all the words where “A” gets bolded.\n\n3\n\nIf the value in the Word column is one of these items\n\n4\n\nThen pass the value of the Word_Label column to str_replace(). Replace the “a” from the regular-face set with the “a” from the bold-face set.\n\n5\n\nIf the value in the Word column is not any of those words, keep the value of Word_Label the same.\n\n6\n\nSame logic for all the words where “E” gets bolded.\n\n7\n\nSame logic for all the words where “I” gets bolded.\n\n8\n\nSame logic for all the words where “O” gets bolded.\n\n9\n\nSame logic for all the words where “U” gets bolded.\n\n10\n\nThere are a couple of exceptions: “believe”, because that’s the word where the second instance of the vowel gets bolded, not the first one. Replace the consecutive “I” and “E” from the regular-face set with the “I” and “E” from the bold-face set.\n\n11\n\n“Goose” is the only word where both O’s need to be bolded.\n\n12\n\n“Fleece” needs the first two, but not the third E bolded.\n\n13\n\nConvert Word_Label from character to factor.\n\n\n\n\nWhich renders as:\n\n𝖠𝖬𝗢𝖭𝖦, 𝖠𝖭𝗢𝖳𝖧𝖤𝖱, 𝖡𝖤𝖫𝗜𝗘𝖵𝖤, 𝖡𝖫𝗨𝗘, 𝖡𝗔𝖫𝖫, 𝖡𝗔𝖭𝖦, 𝖡𝗔𝖳𝖧, 𝖡𝗘𝖳, 𝖡𝗘𝗔𝖳, 𝖡𝗜𝖳, 𝖡𝗢𝗨𝖦𝖧𝖳, 𝖡𝗨𝖳, 𝖢𝖫𝗢𝖳𝖧, 𝖢𝖴𝖱𝖤, 𝖢𝗢𝖱𝖤, 𝖢𝗢𝗨𝖫𝖣, 𝖣𝖱𝗘𝖲𝖲, 𝖥𝖫𝗘𝗘𝖢𝖤, 𝖥𝗔𝖬𝖮𝖴𝖲, 𝖥𝗔𝖳𝖧𝖤𝖱, 𝖥𝗜𝖭𝖨𝖲𝖧, 𝖥𝗢𝖱𝖢𝖤, 𝖥𝗢𝗢𝖳, 𝖥𝗨𝖭, 𝖦𝗢𝗔𝖳, 𝖦𝗢𝗢𝖲𝖤, 𝖦𝗨𝗘𝖲𝖳, 𝖧𝗔𝖭𝖣, 𝖧𝗢𝖭𝖮𝖱𝖨𝖥𝖨𝖢, 𝖪𝗜𝖳, 𝖫𝗔𝗨𝖦𝖧, 𝖫𝗢𝖳, 𝖭𝗘𝗔𝖱, 𝖯𝗔𝖫𝖬, 𝖯𝗘𝗢𝖯𝖫𝖤, 𝖯𝗜𝖭, 𝖯𝗨𝖳, 𝖲𝖰𝗨𝗔𝖱𝖤, 𝖲𝖳𝖱𝗨𝖳, 𝖲𝖳𝗔𝖱𝖳, 𝖲𝗔𝖸𝖲, 𝖲𝗨𝖯𝖯𝖮𝖱𝖳, 𝖳𝖧𝖱𝗢𝗨𝖦𝖧, 𝖳𝖧𝗢𝗨𝖦𝖧𝖳, 𝖳𝖱𝗔𝖯, 𝖶𝖧𝗢, 𝖶𝖱𝗢𝖭𝖦\n\nNow we can see which vowels are being plotted more clearly (but with a reminder of how messy English orthography is):\n\nwords_ep_2 &lt;- formants %&gt;%\n  filter(List == \"Lingthusiasm Episodes\") %&gt;%\n1  group_by(Speaker, Vowel, Word_Label) %&gt;%\n  summarise(F1 = mean(F1), F2 = mean(F2)) %&gt;%\n2  ggplot(aes(x = F2, y = F1, color = Vowel, label = Word_Label)) +\n  geom_point(size = 1.25) +\n  geom_label_repel(min.segment.length = 0, force = 75, seed = 2024, size = 4) +\n  facet_wrap(~Speaker, ncol = 1) +\n  scale_color_manual(values = carto_pal(12, \"Bold\")) +\n  scale_x_reverse(breaks = c(750, 1250, 1750, 2250, 2750)) +\n  scale_y_reverse(breaks = c(250, 500, 750, 1000)) +\n  theme_classic() +\n  lingthusiasm_theme +\n  theme(plot.subtitle = element_markdown(size = 15, family = \"sans\")) +\n  labs(title = \"Means By Word: Lingthusiasm Episodes\", subtitle = vowel_key) +\n  guides(color = guide_none())\n\nwords_ep_2\n\n\n1\n\nReplace Word with Word_Label.\n\n2\n\nReplace Word with Word_Label here too.\n\n\n\n\n\n\n\nFigure 8: Mean for each word in the Lingthusiasm Episode word list.\n\n\n\n\nThis was one of the points where I (from the northeast US) realized how I don’t have a lot of experience with Australian accents, because I wasn’t entirely sure how much of the messiness in Lauren’s back vowel data was because her back vowels are in different locations, or because I picked words where she uses a different vowel than Gretchen and I do.\n\n\n3.5 Plot Vowel Boundaries\nOne of the the most common types of vowel plots draws ellipses around the vowel boundaries.\n\nellipses &lt;- formants %&gt;%\n1  ggplot(aes(x = F2, y = F1, group = Vowel)) +\n2  geom_mark_ellipse(\n3    aes(color = Vowel),\n4    expand = 0\n  ) +\n5  geom_textbox(\n6    data = . %&gt;%\n      summarise(.by = c(Speaker, List, Vowel), F1 = mean(F1), F2 = mean(F2)),\n7    aes(label = Vowel, fill = Vowel),\n8    color = lingthusiasm_navy, size = 4.5, halign = 0.5, valign = 0.5,\n9    alpha = 0.8, box.color = NA,\n10    width = unit(0.10, \"snpc\"), height = unit(0.10, \"snpc\"),\n    box.padding = unit(c(0, 0, 0, 0), \"snpc\"), box.r = unit(0.01, \"snpc\")\n  ) +\n  facet_grid(Speaker ~ List) +\n  scale_color_manual(values = carto_pal(12, \"Bold\")) +\n  scale_fill_manual(values = carto_pal(12, \"Bold\")) +\n  scale_x_reverse(breaks = c(750, 1250, 1750, 2250, 2750)) +\n  scale_y_reverse(breaks = c(250, 500, 750, 1000)) +\n  theme_classic() +\n  lingthusiasm_theme +\n  labs(title = \"Vowel Boundaries\") +\n  guides(color = guide_none(), fill = guide_none())\n\nellipses\n\n\n1\n\nSpecify group = Vowel because all layers are going to be grouped by vowel, but wait to specify fill and color because those will vary by geom.\n\n2\n\nggforce::geom_mark_ellipse() draws an ellipse around the points in each Vowel group. stat_ellipse() is also an option, but not all the vowels in the Wells Lexical Set have enough observations for it.\n\n3\n\nSet the outline around the ellipse to be color-coded by Vowel, but keep the fill of the ellipses white.\n\n4\n\nexpand = 0 draws the ellipse exactly around the edges of the data, vs. the default value of expanding the ellipse out by 5mm.\n\n5\n\nIf you give geom_mark_ellipse() a value for label, it will draw labels for each ellipse. But I want the labels centered on the mean for each vowel, not drawn to the side, so to do that I’m using geom_textbox() again (like for the first vowel means plots).\n\n6\n\ngeom_textbox() needs the means for each Speaker*List*Vowel, otherwise it will draw a label over every data point. We can summarize the data already passed into the plot (formats) by specifying data = . %&gt;% summarise(). So this geom (but not the ellipse geom) uses the mean of F1 and F2 grouped by Speaker, List, and Vowel. If you only specified Vowel here, it would plot the same means on each panel when we facet_wrap() by Speaker and Vowel on the next line.\n\n7\n\nSet the label and fill (but not color) to vary by Vowel for just geom_textbox().\n\n8\n\nSet the text size, alignment, and color.\n\n9\n\nRemove the outlines around the boxes and make the fill color slightly transparent.\n\n10\n\nSet the box dimensions (see vowel means plots above).\n\n\n\n\n\n\n\nFigure 9: Vowel means (labels) and boundaries (ellipses).\n\n\n\n\nEllipse plots are hard to make work for this data set, since there’s just too much going on in the Lingthusiasm Episode data. It would work better with cleaner/more controlled data (like the Wells Lexical Set recordings), or if you weren’t trying to compare all of the vowels. For now, I think this just shows how complex speech comprehension is! There isn’t a straightforward way to visualize boundaries in the naturalistic data, even though we’re able to perceive boundaries without much trouble.\nFor more info on ellipse vowel plots and how to tweak their appearance, I like this tutorial.\nSave the plots created so far:\n\nggsave(\n  means_1, path = \"plots\", filename = \"1_means_original.png\",\n1  width = 8, height = 5, unit = \"in\", device = png\n)\nggsave(\n  means_2, path = \"plots\", filename = \"2_means_flipped.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\nggsave(\n  points, path = \"plots\", filename = \"3_individual_points.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\nggsave(\n  words_ls, path = \"plots\", filename = \"4_words_lexical_set.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\nggsave(\n  words_ep_2, path = \"plots\", filename = \"4_words_episodes.png\",\n  width = 8, height = 8, unit = \"in\", device = png\n)\nggsave(\n  ellipses, path = \"plots\", filename = \"5_ellipses.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nNeed to specify device = png (not leave default or device = \"png\") to get Josefin Sans font to render correctly.\n\n\n\n\n\n\n3.6 Stylized Versions\nThe plots so far would work for a scientific presentation/paper, but the original goal was to make plots that are a little more artistic. Now that we understand what vowel data is going into the plots and how the vowel plots correspond to the IPA vowel chart, we can play around with removing some of the axis and label information for a more minimalist look. Starting at this step, without making the complete plot first, is possible, but it would be easy to end up with errors.\nBasically, if you want to remove axes for scientific presentations: don’t. If you want to remove axes for artistic license: make sure you understand what you’re removing first.\n\nWord Means for Wells Lexical Set\nThe first pair of stylized plots just includes the Wells Lexical Set data.\nggrepel::geom_label_repel(), which draws the label boxes offset from the points, doesn’t currently support having the label text a different color than the box outline, so the trick with this one is to call it twice—once to draw everything in navy, and a second time to draw just the text in green on top of the navy text.\nHere’s Gretchen’s version:\n\n1gretchen_words_ls &lt;- formants %&gt;%\n  filter(Speaker == \"Gretchen\" & List == \"Wells Lexical Set\") %&gt;%\n2  summarise(.by = Word, F1 = mean(F1), F2 = mean(F2)) %&gt;%\n3  ggplot(aes(x = F2, y = F1, label = Word)) +\n4  geom_point(color = lingthusiasm_navy, size = 2) +\n5  geom_label_repel(\n6    color = lingthusiasm_navy,\n7    family = \"sans_alt\", size = 5,\n8    min.segment.length = 0, segment.size = 0.75, label.size = 1,\n9    label.padding = unit(0.4, \"lines\"), label.r = unit(0.4, \"lines\"),\n10    force = 65, seed = 84\n  ) +\n11  geom_label_repel(\n12    color = lingthusiasm_green,\n13    family = \"sans_alt\", size = 5,\n14    min.segment.length = 0, segment.size = NA, label.size = NA,\n15    label.padding = unit(0.4, \"lines\"), label.r = unit(0.4, \"lines\"),\n    force = 65, seed = 84\n  ) +\n16  scale_x_reverse(limits = c(3100, 500), expand = c(0, 0)) +\n  scale_y_reverse(limits = c(1200, 325), expand = c(0, 0)) +\n  theme_classic() +\n17  theme(\n    axis.line = element_blank(), axis.text = element_blank(),\n    axis.ticks = element_blank(), axis.title = element_blank()\n  ) +\n18  annotate(\n    geom = \"text\", label = \"Gretchen\",\n19    family = \"sans_alt\", size = 11, color = lingthusiasm_green,\n20    x = 2525, y = 1070\n  ) +\n21  inset_element(\n    p = lingthusiasm_logo,\n22    left = unit(0.05, \"snpc\"), right = unit(0.15, \"snpc\"),\n    top = unit(0.2, \"snpc\"), bottom = unit(0.1, \"snpc\")\n  )\n\n23ggsave(\n  gretchen_words_ls, path = \"plots\", filename = \"gretchen_words_ls.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nJust include Gretchen’s data from the Wells Lexical Set list.\n\n2\n\nCalculate the mean of F1 and F2 for each word.\n\n3\n\nLike the rest of the plots, F1 goes on the Y axis, F2 goes on the X axis, and Word is the label.\n\n4\n\nDraw a point at each word mean, and make it the Lingthusiasm navy and slightly larger than the default.\n\n5\n\nFirst layer of geom_text_repel() to get the navy lines.\n\n6\n\nMake the text color and the outline around the box navy.\n\n7\n\nJosefin Sans font and larger text size.\n\n8\n\nAlways draw a line between the label and the point, and make the line a bit thicker than default.\n\n9\n\nIncrease the padding around the text and the amount of curve on the corners.\n\n10\n\nIncrease the amount the labels are repelled from the points (play around with different values until it looks good), and set a seed so the output is consistent.\n\n11\n\nSecond layer of geom_text_repel() to get the green text.\n\n12\n\nMake the text color (and also the outline for now) the Lingthusiasm green.\n\n13\n\nJosefin Sans font and larger text size. Needs to be the same as the previous layer so there’s no navy text visible below the green text.\n\n14\n\nSet segment.size and label.size to 0 so that this layer doesn’t draw a green box outline or line connecting to the point.\n\n15\n\nSame box padding and shape, repel value, and seed so that the labels are in the same place.\n\n16\n\nReverse the axes, and set the limits to look right before removing the labels in the next step.\n\n17\n\nRemove the axis lines, ticks, numbers, and title.\n\n18\n\nAdd a layer writing “Gretchen.”\n\n19\n\nSet the font to Josefin Sans, the color to Lingthusiasm green, and the size to be about double the size of the word labels.\n\n20\n\nThe location is on the scale of the axes.\n\n21\n\nUse patchwork to add a layer for the Lingthusiasm logo. This layer needs to be last, and the image needs to be loaded as a raster (see Lingthusiasm Theme section at the beginning).\n\n22\n\nLocations for the logo. (0, 0) is the bottom left corner of the plot, and (1, 1) is the top right corner. The distance between the top/bottom and left/right are even, and the units are snpc (squared normalized parent coordinates), so the logo is square and 10% of the height of the plot.\n\n23\n\nSave the plot. The text/line sizes and locations of the annotation layers are set to look right at this size and aspect ratio. Instead of rendering the plot in this chunk like before, the saved image is inserted directly below, to keep the sizing exact.\n\n\n\n\n\n\n\nFigure 10: Gretchen: Means for each word in the Wells Lexical Set data.\n\n\nAnd here’s Lauren’s data:\n\nlauren_words_ls &lt;- formants %&gt;%\n1  filter(Speaker == \"Lauren\" & List == \"Wells Lexical Set\") %&gt;%\n  summarise(.by = Word, F1 = mean(F1), F2 = mean(F2)) %&gt;%\n  ggplot(aes(x = F2, y = F1, label = Word)) +\n  geom_point(color = lingthusiasm_navy, size = 2) +\n  geom_label_repel(\n    color = lingthusiasm_navy,\n    family = \"sans_alt\", size = 5,\n    min.segment.length = 0, segment.size = 0.75, label.size = 1,\n    label.padding = unit(0.4, \"lines\"), label.r = unit(0.4, \"lines\"),\n    force = 65, seed = 84\n  ) +\n  geom_label_repel(\n    color = lingthusiasm_green,\n    family = \"sans_alt\", size = 5,\n    min.segment.length = 0, segment.size = NA, label.size = NA,\n    label.padding = unit(0.4, \"lines\"), label.r = unit(0.4, \"lines\"),\n    force = 65, seed = 84\n  ) +\n2  scale_x_reverse(limits = c(3000, 500), expand = c(0, 0)) +\n  scale_y_reverse(limits = c(1000, 325), expand = c(0, 0)) +\n  theme_classic() +\n  theme(\n    axis.line = element_blank(), axis.text = element_blank(),\n    axis.ticks = element_blank(), axis.title = element_blank()\n  ) +\n  annotate(\n    geom = \"text\", label = \"Lauren\",\n    family = \"sans_alt\", size = 11, color = lingthusiasm_green,\n3    x = 2500, y = 865,\n  ) +\n  inset_element(\n    p = lingthusiasm_logo,\n4    left = unit(0.05, \"snpc\"), right = unit(0.15, \"snpc\"),\n    top = unit(0.25, \"snpc\"), bottom = unit(0.15, \"snpc\")\n  )\n\nggsave(\n  lauren_words_ls, path = \"plots\", filename = \"lauren_words_ls.png\",\n  width = 8, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nChange to include Lauren’s data instead of Gretchen’s.\n\n2\n\nTo get a plot where the edge around the data are even, the limits for Lauren’s data are slightly different than the limits for Gretchen’s.\n\n3\n\nThe location of the name layer also needs to be slightly different for Lauren.\n\n4\n\nAnd finally so does the location of the logo (but the size/aspect ratio stay the same).\n\n\n\n\n\n\n\nFigure 11: Lauren: Means for each word in the Wells Lexical Set data.\n\n\n\n\nVowel Means for Lingthusiasm Episode Words\nThe second pair of stylized plots uses the data from the Lingthusiasm episodes.\nI want to draw a trapezoid around vowel space, with the axes the same for Gretchen and Lauren’s data so you can compare them. geom_path() connects a series of points, so we need the four corners, plus the starting corner again. Here’s a visualization of the trapezoid coordinates, relative to the vowel means:\n\nvowel_polygon &lt;- tibble(\n  x = c(800, 800, 3000, 2000, 800),\n  y = c(1000, 275, 275, 1000, 1000)\n)\n\nformants %&gt;%\n  summarise(.by = c(Vowel, Speaker), F1 = mean(F1), F2 = mean(F2)) %&gt;%\n  ggplot(aes(x = F2, y = F1)) +\n  geom_path(data = vowel_polygon, aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  theme_classic() +\n  annotate(\n    geom = \"label\", label = \"1/5\",\n    x = as.numeric(vowel_polygon[1, \"x\"]), y = as.numeric(vowel_polygon[1, \"y\"])\n  ) +\n  annotate(\n    geom = \"label\", label = \"2\",\n    x = as.numeric(vowel_polygon[2, \"x\"]), y = as.numeric(vowel_polygon[2, \"y\"])\n  ) +\n  annotate(\n    geom = \"label\", label = \"3\",\n    x = as.numeric(vowel_polygon[3, \"x\"]), y = as.numeric(vowel_polygon[3, \"y\"])\n  ) +\n  annotate(\n    geom = \"label\", label = \"4\",\n    x = as.numeric(vowel_polygon[4, \"x\"]), y = as.numeric(vowel_polygon[4, \"y\"])\n  )\n\n\n\n\nNow that we have the layout of the trapezoid, plot Gretchen’s data. geom_textbox() and geom_label_repel() can only draw squares/rectangles, so to get circles like the Lingthusiasm logo, I use geom_mark_circle() to draw a circle, then geom_text() to write the vowel on top of it.\n\n1gretchen_vowels_ep &lt;- formants %&gt;%\n  filter(Speaker == \"Gretchen\" & List == \"Lingthusiasm Episodes\") %&gt;%\n  summarise(.by = Vowel, F1 = mean(F1), F2 = mean(F2)) %&gt;%\n2  ggplot() +\n3  geom_path(\n    data = vowel_polygon,\n    aes(x = x, y = y),\n4    color = lingthusiasm_navy, linewidth = 2, lineend = \"round\"\n  ) +\n5  geom_mark_circle(\n    aes(x = F2, y = F1, group = Vowel),\n6    radius = unit(0.048, \"snpc\"), n = 1000,\n7    fill = lingthusiasm_green, alpha = 1, color = NA\n  ) +\n8  geom_text(\n    aes(x = F2, y = F1, label = Vowel),\n9    color = \"white\", size = 11, nudge_y = 4\n  ) +\n  scale_x_reverse(\n10    limits = c(max(vowel_polygon$x), min(vowel_polygon$x)),\n    expand = c(0.01, 0.01)\n  ) +\n  scale_y_reverse(\n    limits = c(max(vowel_polygon$y), min(vowel_polygon$y)),\n    expand = c(0.01, 0.01),\n11    position = \"right\"\n  ) +\n  theme_classic() +\n  theme(\n12    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n13    axis.title.y = element_text(\n      size = 30, hjust = 1,\n      color = lingthusiasm_navy, family = \"sans_alt\"\n    )\n  ) +\n14  labs(y = \"Gretchen\")\n\nggsave(\n  gretchen_vowels_ep, path = \"plots\", filename = \"gretchen_vowels_ep.png\",\n  width = 7.25, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nJust include Gretchen’s data from the Lingthusiasm episode word list, and calculate the mean F1 and F2 for each Vowel.\n\n2\n\nInstantiate the plot, but set the values for aes() separately in each geom below.\n\n3\n\nDraw the trapezoid, using the data from vowel_polygon not the vowel means passed into the plot. This draws a line connecting each of the coordinates listed in the x and y columns.\n\n4\n\nMake the line navy, thicker than default, and rounded at the corners.\n\n5\n\nDraw a circle around each vowel mean. Specify group = Vowel not label = Vowel, so that there is a circle for each vowel, but no labels.\n\n6\n\nMake the radius of the circle 4.8% of the plot’s height (again using squared normalized parent coordinates, just small enough to not overlap), and increase the number of points used to draw the circle from 100 so it looks smoother.\n\n7\n\nFill the circle with Lingthusiasm green, make it not transparent (the default alpha is 0.3 not 1), and remove the black outline.\n\n8\n\nDraw the vowel label at each mean.\n\n9\n\nMake the text white, large enough to just fit inside the circle, and nudge it up just a bit (because most of the IPA symbols are sized as lowercase).\n\n10\n\nSet the axis limits to be 1% larger than the borders of the trapezoid.\n\n11\n\nMove the title of the Y axis from the left to the right.\n\n12\n\nRemove the lines, text, and ticks from the X and Y axes and the title from the X axis.\n\n13\n\nMake the Y axis title size 30, aligned to the bottom not the middle, navy, and Josefin Sans.\n\n14\n\nSet the Y axis title to be Gretchen instead of F1.\n\n\n\n\n\n\n\nFigure 12: Gretchen: Means for each vowel in the Lingthusiasm episode data.\n\n\nDo the same thing for Lauren’s data:\n\nlauren_vowels_ep &lt;- formants %&gt;%\n1  filter(Speaker == \"Lauren\" & List == \"Lingthusiasm Episodes\") %&gt;%\n  summarise(.by = Vowel, F1 = mean(F1), F2 = mean(F2)) %&gt;%\n  ggplot(aes(x = F2, y = F1, group = Vowel)) +\n  geom_path(\n    data = vowel_polygon,\n    aes(x = x, y = y, group = NA),\n    color = lingthusiasm_navy, linewidth = 2, lineend = \"round\"\n  ) +\n  geom_mark_circle(\n    radius = unit(0.048, \"snpc\"), n = 1000,\n    fill = lingthusiasm_green, alpha = 1, color = NA\n  ) +\n  geom_text(aes(label = Vowel), color = \"white\", size = 11, nudge_y = 4) +\n  scale_x_reverse(\n    limits = c(max(vowel_polygon$x), min(vowel_polygon$x)),\n    expand = c(0.01, 0.01)\n  ) +\n  scale_y_reverse(\n    limits = c(max(vowel_polygon$y), min(vowel_polygon$y)),\n    expand = c(0.01, 0.01),\n    position = \"right\"\n  ) +\n  theme_classic() +\n  theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(\n      size = 30, hjust = 1,\n      color = lingthusiasm_navy, family = \"sans_alt\"\n    )\n  ) +\n2  labs(y = \"Lauren\")\n\nggsave(\n  lauren_vowels_ep, path = \"plots\", filename = \"lauren_vowels_ep.png\",\n  width = 7.25, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nOnly things that need to change from Gretchen’s version are the filter on the data.\n\n2\n\nAnd the axis label.\n\n\n\n\n\n\n\nFigure 13: Lauren: Means for each vowel in the Lingthusiasm episode data.\n\n\nTo combine the two, I’m going to flip Gretchen’s horizontally, so it looks like two speakers facing each other.\n\n1paired_vowels_ep &lt;- gretchen_vowels_ep +\n2  scale_x_continuous(\n    limits = c(min(vowel_polygon$x), max(vowel_polygon$x)),\n    expand = c(0.01, 0.01)\n  ) +\n  scale_y_reverse(\n    limits = c(max(vowel_polygon$y), min(vowel_polygon$y)),\n    expand = c(0.01, 0.01),\n3    position = \"left\"\n  ) +\n  theme(\n4    axis.title.y = element_text(hjust = 0),\n5    plot.margin = margin(t = 10, l = 10, r = 30, b = 10)\n  ) +\n6  lauren_vowels_ep +\n7  theme(plot.margin = margin(t = 10, l = 30, r = 10, b = 10)) +\n8  lingthusiasm_tagline +\n9  plot_layout(design = c(\n10    area(t = 1, l = 1, b = 5, r = 4),\n11    area(t = 1, l = 5, b = 5, r = 8),\n    area(t = 4, l = 4, b = 5, r = 5)\n  ))\n\nggsave(\n  paired_vowels_ep, path = \"plots\", filename = \"paired_vowels_ep.png\",\n  width = 15, height = 5, unit = \"in\", device = png\n)\n\n\n1\n\nUse the patchwork package to combine plots. Start with Gretchen’s plot on the left.\n\n2\n\nReplace the x axis scale with the non-reversed version scale_x_continuous, flip the limits accordingly, and keep the expansion the same.\n\n3\n\nKeep the y axis scale reversed, but put the axis title back on the left.\n\n4\n\nShift the Y axis title (“Gretchen”) to be aligned to the bottom.\n\n5\n\nAdd some space to the right side of the plot.\n\n6\n\nAdd Lauren’s plot.\n\n7\n\nAdd space to the left side of the plot. Based on how patchwork works, this only applies to Lauren’s plot added last, not Gretchen’s.\n\n8\n\nAdd the Lingthusiasm tagline image.\n\n9\n\nSpecify the locations of the three pieces on a grid to have Gretchen’s plot on the left, Lauren’s on the right, and the Lingthusiasm logo on the bottom.\n\n10\n\nGretchen’s plot: full height, left side.\n\n11\n\nLauren’s plot: full height, right side. 12 Lingthusiasm tagline logo: 20% of height, overlapping each of the other plots by 25%.\n\n\n\n\n\n\n\nFigure 14: Gretchen (left) & Lauren (right): Means for each vowel in the Lingthusiasm episode data.\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2024,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2024-02-07},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2024. “Lingthusiasm Vowel Plots.”\nFebruary 7, 2024. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  }
]