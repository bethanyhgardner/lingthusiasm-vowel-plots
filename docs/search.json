[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lingthusiasm Vowel Plots",
    "section": "",
    "text": "Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "2_annotate_audio.html",
    "href": "2_annotate_audio.html",
    "title": "Part 2: Annotating the Audio",
    "section": "",
    "text": "2.1 Setup\n\n\"\"\"Part 2 of Lingthusiasm Vowel Plots: Trimming Audio and Getting Vowel Formants.\"\"\"\n\n1import glob\nimport os\n2import pandas as pd\n3from pytube import Playlist, YouTube\n4from pydub import AudioSegment\n5import parselmouth\n\n\n1\n\nFile utilities.\n\n2\n\nDataframes.\n\n3\n\nGetting captions and audio data from YouTube.\n\n4\n\nWorking with audio files.\n\n5\n\nInterface with Praat.\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "1_find_words.html",
    "href": "1_find_words.html",
    "title": "Part 1: Finding Vowels to Plot",
    "section": "",
    "text": "There are going to be two sources of data, one more naturalistic and one more controlled:\n\nVowels pulled from the Lingthusiasm episode recordings\nVowels from Gretchen & Lauren recording the Wells lexical set\n\nThe first steps are to find the words for #1, and we’ll come back to #2 later.\n\n1.1 Setup\n\n\"\"\"Part 1 of Lingthusiasm Vowel Plots: Finding Vowels to Annotate.\"\"\"\n\n1import re\n2import pandas as pd\n3import requests\nfrom bs4 import BeautifulSoup\n4from thefuzz import process\n5from pytube import Playlist, YouTube\n\n\n1\n\nRegex functions.\n\n2\n\nDataframe functions.\n\n3\n\nScraping data from webpages.\n\n4\n\nFuzzy string matching.\n\n5\n\nGetting captions and audio from YouTube.\n\n\n\n\n(Note: All of this could also be done in R, but I’ve done it in Python here primarily because there are Python packages that can get data from YouTube without having to set up anything on the YouTube API side. Here, all you need to do is install the package.)\n\n\n1.2 Get Transcript Text & Speakers\n\n1.2.1 List Episodes\nThe Lingthusiasm website has a page listing all of the available transcripts. Step 1 is to load that page and get the list of URLs to the transcripts. (They have similar but not identical structures, so it’s easiest to read the list from the website instead of trying to construct them here.)\nThis function uses the BeautifulSoup package to return an HTML object and a text string for a URL:\n\ndef get_html_text(url):\n    \"\"\"Use BeautifulSoup to get the webpage text from the URL.\"\"\"\n\n1    resp = requests.get(url, timeout=1000)\n2    html = BeautifulSoup(resp.text, 'html.parser')\n\n3    return {'html': html, 'text': html.get_text()}\n\n\n1\n\nConnect to webpage.\n\n2\n\nLoad the HTML data from the webpage.\n\n3\n\nReturn the HTML data object and the text from the HTML data object.\n\n\n\n\nThis function uses BeautifulSoup to filter just the transcript URLs from the HTML data:\n\ndef get_transcript_links(url):\n    \"\"\"Get URLs to episode transcripts from HTML page.\"\"\"\n\n1    html = get_html_text(url)['html']\n2    url_objs = html.find_all('a')\n3    urls = [l.get('href') for l in url_objs]\n4    urls = pd.Series(data=urls, name='URL', dtype='string')\n5    urls = urls[urls.str.contains('transcript')]\n6    urls = urls[::-1]\n\n7    return urls.reset_index(drop=True)\n\n\n1\n\nGet HTML data from webpage using function defined above.\n\n2\n\nFilter using the a tag to get all of the link items.\n\n3\n\nGet the href item from each a tag item, which is the text of the URL.\n\n4\n\nConvert from list to pandas series.\n\n5\n\nFilter to only include URLs including the word transcript.\n\n6\n\nSort to have earliest episodes first.\n\n7\n\nReturn with index (row numbers) reset.\n\n\n\n\nThere are 84 episodes available (as of early October 2023):\n\n1transcript_urls = get_transcript_links('https://lingthusiasm.com/transcripts')\n\n2transcript_urls.head().to_frame().style\n\n\n1\n\nGet the URLs for the episode transcripts from the table of contents page, using the functions defined above.\n\n2\n\nTake the first 10 rows of the resulting list and print it as a nice table.\n\n\n\n\n\n\n\n\n\n \nURL\n\n\n\n\n0\nhttps://lingthusiasm.com/post/155357756341/transcript-lingthusiasm-episode-1-speaking-a\n\n\n1\nhttps://lingthusiasm.com/post/156181768226/transcript-lingthusiasm-episode-2-pronouns\n\n\n2\nhttps://lingthusiasm.com/post/157167562811/transcript-lingthusiasm-episode-3-arrival-of-the\n\n\n3\nhttps://lingthusiasm.com/post/157268108811/transcript-lingthusiasm-episode-4-inside-the-word\n\n\n4\nhttps://lingthusiasm.com/post/158014366301/transcript-lingthusiasm-episode-5-colour-words\n\n\n\n\n\nOnly keep episodes 1-84, for consistency replicating this code later:\n\ntranscript_urls = transcript_urls[:84]\n\n\n\n1.2.2 Scrape Transcript Text\nNow we can download transcripts, which are split into turns and labelled by speaker.\nSplitting out some sub-tasks into separate methods, this function gets the episode number (following episode-) as an integer from the URL:\n\ndef transcript_number_from_url(url):\n    \"\"\"Find transcript number in URL.\"\"\"\n\n1    index_episode = url.find('episode-')\n2    cur_number = url[index_episode + 8:]\n3    index_end = cur_number.find('-')\n    if index_end != -1:\n        cur_number = cur_number[:index_end]\n\n4    return int(cur_number)\n\n\n1\n\nFind location of episode-, since episode number is immediately following it.\n\n2\n\nSubset URL starting 8 characters after start of episode- string, which is immediately after it.\n\n3\n\nMost of the transcript URLs have more text after the number. If so, trim to just keep the number.\n\n4\n\nReturn episode number converted from string to integer.\n\n\n\n\nThe text returned from the transcript pages has information at the top and bottom that we don’t need. \"This is a transcript for\" marks the start of the transcript, and \"This work is licensed under\" marks the end, so subset the transcript dataframe rows to only include that section:\n\ndef trim_transcript(df):\n    \"\"\"Find start and end of transcript text.\"\"\"\n\n    start_index = df.find('This is a transcript for')\n    end_index = df.find('This work is licensed under')\n\n    return df[start_index:end_index]\n\nThis function cleans up the text column so it plays a bit nicer with Excel CSV export:\n\ndef clean_text(l):\n    \"\"\"Clean text column so it opens correctly as Excel CSV.\"\"\"\n\n1    l = l.strip()\n2    l = l.replace('\\u00A0', ' ').replace('–', '--').replace('…', '...')\n3    l = re.sub('“|”|‘|’', \"'\", l)\n\n4    return ' '.join(l.split())\n\n\n1\n\nRemove leading and trailing whitespaces.\n\n2\n\nReplace non-breaking spaces, en dashes, and ellipses.\n\n3\n\nReplace slanted quotes.\n\n4\n\nRemove double spaces between words.\n\n\n\n\nAfter a bit of trial and error, it’s easiest to split on the speaker names, since the paragraph formatting isn’t identical across all the pages:\n\nspeaker_names = [\n    'Gretchen', 'Lauren', 'Ake', 'Bona', 'Ev', 'Fei Ting', 'Gabrielle',\n    'Jade', 'Hannah', 'Hilaria', 'Janelle', 'Kat', 'Kirby', 'Lina', 'Nicole',\n    'Pedro', 'Randall', 'Shivonne', 'Suzy'\n]\n1speaker_regex = '(' + ':)|('.join(speaker_names) + ':)'\n\n\n1\n\nRegex looks like (Name1:)|(Name2:) etc. Surrounding names in parentheses makes it a capture group, so the names are included in the list of split items instead of dropped.\n\n\n\n\nThis function puts it all together to read the transcripts into one dataframe:\n\ndef get_transcripts(urls):\n    \"\"\"Get transcript text from URLs and format into dataframe.\"\"\"\n\n1    df = []\n    for l in urls:\n2        cur_text = get_html_text(l)['text']\n3        cur_text = trim_transcript(cur_text)\n4        cur_lines = re.split(speaker_regex, cur_text)\n5        cur_lines = [l for l in cur_lines if l is not None]\n6        cur_lines = cur_lines[1:]\n7        speakers = cur_lines[::2]\n        turns = cur_lines[1::2]\n8        cur_df = pd.DataFrame({\n            'Episode': transcript_number_from_url(l),\n            'Speaker': speakers,\n            'Text': [clean_text(line) for line in turns],\n        })\n        cur_df['Speaker'] = cur_df['Speaker'].str.removesuffix(':')\n9        cur_df['Turn'] = cur_df.index + 1\n10        cur_df = cur_df.set_index(['Episode', 'Turn'], drop=True)\n11        df.append(cur_df)\n\n12    df = pd.concat(df)\n13    df['Speaker'] = df['Speaker'].astype('category')\n    df['Text'] = df['Text'].astype('string')\n\n    return df\n\n\n1\n\nMake list to store results.\n\n2\n\nRead text string (vs HTML object) from URL, using function defined above.\n\n3\n\nTrim to only include transcript section, using function defined above.\n\n4\n\nSplit string into list with one item for each speaker turn, using regex defined above.\n\n5\n\nDrop items in list that are None (which are included because of capture group syntax).\n\n6\n\nDrop the initial “This is a transcript for…” line.\n\n7\n\nNow all the odd items are speaker labels and even items are turn text. [::2] is every 2nd item starting at 0, and [1::2] is every 2nd item starting at 1.\n\n8\n\nClean up strings and put everything into a dataframe.\n\n9\n\nMake column for turn number, which is index + 1.\n\n10\n\nSet Episode and Turn columns as indices.\n\n11\n\nAdd to list of parsed episodes.\n\n12\n\nCombine list of dataframes into one dataframe. This works because the Episode + Turn index combination is unique.\n\n13\n\nSet datatypes explicitly, to avoid warnings later.\n\n\n\n\nThis takes a minute or so to run:\n\n1transcripts = get_transcripts(transcript_urls)\n\n2pd.concat([transcripts.head(), transcripts.tail()]).style\n\n\n1\n\nRun get_transcripts() on the lists of links to each transcript, defined above.\n\n2\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \n \nSpeaker\nText\n\n\nEpisode\nTurn\n \n \n\n\n\n\n1\n1\nLauren\nWelcome to Lingthusiasm! A podcast that's enthusiastic about linguistics. I'm Lauren Gawne.\n\n\n2\nGretchen\nAnd I'm Gretchen McCulloch. And today we're going to be talking about universal language and why it doesn't work. But first a little bit about what Lingthusiasm is.\n\n\n3\nLauren\nSo I guess the important thing is to unpack 'a podcast that's enthusiastic about linguistics'.\n\n\n4\nGretchen\nYeah! We chose our tagline because we're here to explore interesting things that language has to offer and especially what looking at language from a linguistics perspective can tell us about how language works.\n\n\n5\nLauren\nBoth of us have blogs where we do a fair amount of that, but that's a fairly solitary enterprise and we wanted to take the opportunity to have more of a conversation -- something that's a little less disembodied\n\n\n84\n184\nLauren\nAh, well, that's okay, because now we're at the end of this episode, I'm pointing to everyone. [Music]\n\n\n185\nLauren\nFor more Lingthusiasm and links to all the things pointed to in this episode, go to lingthusiasm.com. You can listen to us on Apple Podcasts, Google Podcasts, Spotify, SoundCloud, YouTube, or wherever else you get your podcasts. You can follow @lingthusiasm on Twitter, Facebook, Instagram, and Tumblr. You can get 'Etymology isn't Destiny' on t-shirts and tote bags and lots of other items, and aesthetic IPA posters, and other Lingthusiasm merch at lingthusiasm.com/merch. I tweet and blog as Superlinguo.\n\n\n186\nGretchen\nI can be found as @GretchenAMcC on Twitter, my blog is AllThingsLinguistic.com, and my book about internet language is called Because Internet. Lingthusiasm is able to keep existing thanks to the support of our patrons. If you wanna get an extra Lingthusiasm episode to listen to every month, our entire archive of bonus episodes to listen to right now, or if you just wanna help keep the show running ad-free, go to patreon.com/lingthusiasm or follow the links from our website. Patrons can also get access to our Discord chatroom to talk with other linguistics fans and be the first to find out about new merch and other announcements. Recent bonus topics include interviews with Sarah Dopierala and Martha Tsutsui-Billins about their own linguistic research and their work on Lingthusiasm and a very special Lingthusiasmr episode where we read The Harvard Sentences to you [ASMR voice] in a calm, soothing voice. [Regular voice] Can't afford to pledge? That's okay, too. We also really appreciate it if you can recommend Lingthusiasm to anyone in your life who's curious about language.\n\n\n187\nLauren\nLingthusiasm is created and produced by Gretchen McCulloch and Lauren Gawne. Our Senior Producer is Claire Gawne, our Editorial Producer is Sarah Dopierala, our Production Assistant is Martha Tsutsui-Billins, and our Editorial Assistant is Jon Kruk. Our music is 'Ancient City' by The Triangles.\n\n\n188\nGretchen\nStay lingthusiastic! [Music]\n\n\n\n\n\nSave results to data/transcripts.csv:\n\ntranscripts.to_csv('data/transcripts.csv', index=True)\n\n\n\n\n1.3 Find Words For Each Vowel\n\n1.3.1 Which Words To Use?\nThese words aren’t all as controlled as you’d probably want for an experiment, but they’re high frequency enough that there are multiple tokens for each one in the episodes.\n\n1word_list = {\n2    '\\u0069': {   # i\n3        'beat': r'\\bbeat\\b',\n        'believe': r'\\bbelieve\\b',\n        'people': r'\\bpeople\\b'},\n    '\\u026A': {  # ɪ\n        'bit': r'\\bbit\\b',\n4        'finish': r'\\bfinish',\n        'pin': r'\\bpin\\b'},\n    '\\u025B': {  # ɛ\n        'bet': r'\\bbet\\b',\n5        'guest': r'\\bguest',\n        'says': r'\\bsays\\b'},\n    '\\u00E6': {  # æ\n        'bang': r'\\bbang\\b',\n        'hand': r'\\bhand\\b',\n        'laugh': r'\\blaugh\\b'},\n    '\\u0075': {  # u\n        'blue': r'\\bblue\\b',\n        'through': r'\\bthrough\\b',\n        'who': r'\\bwho\\b'},\n    '\\u028A': {  # ʊ\n        'could': r'\\bcould\\b',\n        'foot': r'\\bfoot\\b',\n        'put': r'\\bput\\b'},\n    '\\u0254': {  # ɔ\n        'bought': r'\\bbought\\b',\n        'core': r'\\bcore\\b',\n        'wrong': r'\\bwrong\\b'},\n    '\\u0251': {  # ɑ\n        'ball': r'ball\\b|\\bballs\\b',\n        'father': r'\\bfather\\b',\n6        'honorific': r'\\bhonorific'},\n    '\\u028C': {  # ʌ\n        'another': r'\\banother\\b',\n        'but': r'\\bbut\\b',\n        'fun': r'\\bfun\\b'},\n    '\\u0259': {  # ə\n        'among': r'\\bamong\\b',\n        'famous': r'\\bfamous\\b',\n        'support': r'\\bsupport\\b'}\n}\n\n\n1\n\nSet up a nested dictionary, where the outer keys are the IPA vowels, the inner keys are the words, and the values are the regex strings for the words.\n\n2\n\n\\u makes it a unicode string.\n\n3\n\n\\b means search at word boundaries, and only a few words here also include suffixes (e.g., only return results for bit, but return results for finish, finishes, and finishing).\n\n4\n\nAlso get finishes/finishing.\n\n5\n\nAlso get guests.\n\n6\n\nAlso get honorifics. Using honorific instead of honor because there’s an episode about honorifics.\n\n\n\n\n\n\n1.3.2 Find Words In Transcript\nThis function goes through the transcript dataframe and finds turns by Gretchen or Lauren containing the target words:\n\ndef filter_for_words(words, df_all):\n    \"\"\"Find turns by Gretchen/Lauren that contain the target words.\"\"\"\n\n1    df_gl = df_all[\n        (df_all['Speaker'] == \"Gretchen\") |\n        (df_all['Speaker'] == \"Lauren\")\n    ]\n\n2    df_words = pd.DataFrame()\n3    for vowel, examples in words.items():\n4        for word, reg in examples.items():\n5            has_word = df_gl[\n                df_gl['Text'].str.contains(pat=reg, flags=re.IGNORECASE)\n            ]\n6            has_word.insert(0, 'Vowel', vowel)\n            has_word.insert(1, 'Word', word)\n            has_word.set_index(['Word', 'Vowel'], inplace=True, append=True)\n            has_word.index = has_word.index.reorder_levels(\n                ['Vowel', 'Word', 'Episode', 'Turn'])\n\n7            df_words = pd.concat([df_words, has_word])\n\n8    return df_words.sort_index()\n\n\n1\n\nFilter to only include rows where Speaker is Gretchen or Lauren, not guests.\n\n2\n\nCreate dataframe to store results.\n\n3\n\nLoop through outer layer of dictionary, where keys are vowels and values are lists of example words.\n\n4\n\nLoop through inner dictionaries, where keys are the words and values are the regexes.\n\n5\n\nFilter to only include rows where Text column matches word regex. re.IGNORECASE means the search is not case-sensitive.\n\n6\n\nMake columns for the current Vowel and Word, then add them to the index. The results dataframe is now indexed by unique combinations of Vowel, Word, Episode, and Turn.\n\n7\n\nAdd results for current word to dataframe of all results.\n\n8\n\nSorting the dataframe indices makes search faster later, and pandas will throw warnings if you search this large-ish dataframe before sorting.\n\n\n\n\nNext, this function trims the conversation turn around the target word, so it can be matched to the caption timestamps later:\n\ndef trim_turn(df):\n    \"\"\"Find location of target word in conversation turn and subset -/+ 25 characters around it.\"\"\"\n\n1    df['Text_Subset'] = pd.Series(dtype='string')\n\n2    for vowel, word, episode, turn in df.index:\n3        text_full = df.loc[vowel, word, episode, turn]['Text']\n4        word_loc = re.search(re.compile(word, re.IGNORECASE), text_full)\n        word_loc = word_loc.span()[0]\n5        sub_start = max(word_loc - 25, 0)\n6        sub_end = min(word_loc + 25, len(text_full))\n7        text_sub = text_full[sub_start:sub_end]\n        df.loc[(vowel, word, episode, turn), 'Text_Subset'] = str(text_sub)\n\n    return df\n\n\n1\n\nMake a new column for the subset text, so it can be specified as a string. If you insert values later without initializing the column as a string, pandas will throw warnings.\n\n2\n\nGo through each row of the dataframe with all the transcript turns by Gretchen or Lauren that contain target words.\n\n3\n\nUse index values to get the value of Text.\n\n4\n\nSearch for the current row’s Word in the current row’s Text (again case-insensitive). This returns a search object, and the first item in that tuple is the location of the first letter of Word in Text.\n\n5\n\nStart index of Text_Subset is 25 characters before the start of the Word, or the beginning of the string, whichever is larger.\n\n6\n\nEnd index of Text_Subset is 25 characters after the start of the Word, or the end of the string, whichever is smaller.\n\n7\n\nInsert Text_Subset into dataframe.\n\n\n\n\nSubset the transcripts dataframe to only include turns by Gretchen/Lauren that contain a target word:\n\n1transcripts_subset = filter_for_words(word_list, transcripts)\ntranscripts_subset = trim_turn(transcripts_subset)\n\n2pd.concat([transcripts_subset.head(), transcripts_subset.tail()]).style\n\n\n1\n\nRun the two functions just defined on the dataframe of all the transcripts, to filter to only include turns by Gretchen or Lauren that contain the target words, then trim the text of each turn around the target word.\n\n2\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \n \n \n \nSpeaker\nText\nText_Subset\n\n\nVowel\nWord\nEpisode\nTurn\n \n \n \n\n\n\n\ni\nbeat\n1\n119\nGretchen\nno, but you like, you just beat the potato but you don't necessarily deep-fried it you could just pan-fry it.\n, but you like, you just beat the potato but you d\n\n\n12\n86\nGretchen\nYeah, we make it into something like what we have. But another example is, so the sound /ɪ/ as in 'bit,' is not a sound that French has. So you'll have French speakers saying -- well, it kind of has it, but not really -- so if you have something like 'beat' versus 'bit,' for French speakers those are both just kind of 'beat.'\nyou have something like 'beat' versus 'bit,' for F\n\n\n18\n68\nGretchen\nAnd so you have your, like, duh-DUH beat, your iamb, with weak-strong --\nhave your, like, duh-DUH beat, your iamb, with wea\n\n\n100\nGretchen\nAnd the other translators tend to render it in prose, or in, like, shortened lines, but without paying attention to that beat in the same sort of way.\npaying attention to that beat in the same sort of\n\n\n22\n208\nLauren\nYep. I've already used the word 'proximal' in this episode so you're gonna beat me.\nepisode so you're gonna beat me.\n\n\nʌ\nfun\n82\n127\nGretchen\nYes, we did. Because it's sometimes when you know that you're gonna need a whole bunch of examples in a text or an episode, it's fun to theme them so that you're not just reaching for the same -- like I think we used examples like 'I like cake' a lot, or like, 'I eat ice cream.' A lot of our examples are about ice cream and cake, which is fun. I mean, we do like both of these things, but sometimes, for an episode that's gonna be really example heavy, using horses and farmyard animals and stuff is a fun way to make it a little more distinct from other episodes.\ntext or an episode, it's fun to theme them so that\n\n\n137\nGretchen\nThat was a deliberate choice back in the day. That was a style guide thing. Undoing that, now that it's become this unconscious thing that people are still doing, is more challenging. The fun thing, I guess, about how a lot of example sentences in old school syntax papers use 'John' and 'Mary' and 'Bill' is that there is someone who took a bunch of example sentences from papers since the refer to the same people and stitched them together into a single narrative about the adventures of John and Mary and Bill.\nis more challenging. The fun thing, I guess, about\n\n\n83\n1\nGretchen\nWelcome to Lingthusiasm, a podcast that's enthusiastic about linguistics! I'm Gretchen McCulloch. I'm here with Dr. Pedro Mateo Pedro who's an Assistant Professor at the University of Toronto, Canada, a native speaker of Q'anjob'al, and a learner of Kaqchikel. Today, we're getting enthusiastic about kids acquiring Indigenous languages. But first, some announcements. We love looking up whether two words that look kind of similar are actually historically related, but the history of a word doesn't have to define how it's used today. To celebrate how we can grow up to be more than we ever expected, we have new merch that says, 'Etymology isn't Destiny.' Our artist, Lucy Maddox, has made 'Etymology isn't Destiny' into a swoopy, cursive design with a fun little destiny star on the dot of the eye, available in black, white, and my personal favourite, rainbow gradient. This design is available on lots of different colours and styles of shirts. We've got hoodies, tank tops, t-shirts in classic fit, relaxed fit, curved fit -- plus mugs, notebooks, stickers, water bottles, zipper pouches. You know, if it's on Redbubble, we might've put 'Etymology isn't Destiny' on it. We also have tons of other lingthusiastic merch available in our merch store at lingthusiasm.com/merch. I have to say, it makes a great gift to give to a linguistics enthusiast in your life or to request as a gift if you are that linguistics enthusiast. We also wanna give a special shoutout to our aesthetic redesign of the International Phonetic Alphabet. Last year, we reorganised the classic IPA chart to have colours and have little cute circles and not just be boring grey lines of boxes and to even more elegantly represent the principle that the location of the symbols and rows and columns represents the place and degree of constriction in the mouth. I think it looks really cool. It's also a fun little puzzle to sit there and figure out which of the specific circles around different things stands for what. We've now made this aesthetic IPA chart redesign available on lots more merch options, including several different sizes of posters from small ones you can put on a corkboard to large ones you can put up in your hallway. They look really, really good, especially if you have some sort of office-y space that needs to be decorated. Plus, it's on tote bags and notebooks and t-shirts. If you want everyone you meet to know that you're a giant linguistics nerd, you can take them to conferences and use them to start nerdy conversations with people. If you like the idea of linguistics merch but none of ours so far is quite hitting your aesthetic, or if there's an item that Redbubble sells that you think one of our existing designs would look good on, we've added quite a few merch items in response to people's requests over the years, so we'd love to know where the gaps still are and keep an eye on lingthusiasm.com/merch. Our most recent bonus episode was a behind-the-scenes interview with Sarah Dopierala, who you may recognise as a name from the end credits, about what it's like doing transcripts from a linguistics perspective and her life generally as a linguistics grad student. You can go to patreon.com/lingthusiasm to get access to all of the many bonus episodes and to help Lingthusiasm keep running. [Music]\ny, cursive design with a fun little destiny star o\n\n\n84\n167\nGretchen\nI find this a fun and interesting way of thinking about meaning because the way that I think we're sometimes introduced to meaning in a formal context is through dictionary definitions that give you a description of a cat that says something like, 'A cat is a furry creature with four legs, a long tail, purrs,' whatever other attributes you wanna assign to a cat. But in practice, I didn't learn the word 'cat' by looking it up in a dictionary or having a description provided to me. I learned the word 'cat' by having a bunch of cats pointed out to me. Sometimes, a cat has three legs or is one of those weird hairless cats.\nI find this a fun and interesting way o\n\n\n173\nGretchen\nWe did a whole bonus episode about meaning and the ways that we interact with it talking about the 'Is a pizza a sandwich?' meme, which I would highly recommend because I think it's a very fun bonus episode. It turns out that this is actually how meaning works for most people in context. You see a bunch of examples of cats or birds or chairs or whatever. You come up with a sense of what's in the cluster based on generalising from those examples rather than by having an exact list of parameters because some stuff is an exception to those parameters, and it's still a point in the cluster.\nause I think it's a very fun bonus episode. It tur\n\n\n\n\n\nHere’s how many tokens we have for each speaker and word:\n\ntranscripts_subset \\\n    .value_counts(['Vowel', 'Word', 'Speaker'], sort=False) \\\n    .to_frame('Tokens') \\\n    .reset_index(drop=False) \\\n    .style \\\n1    .hide()\n\n\n1\n\nCount the number of items in each combination of Vowel, Word, and Speaker. Convert those results to a dataframe and call the column of counts Tokens. Change Vowel, Word, and Speaker from indices to columns. Print the results as a table, not including row numbers.\n\n\n\n\n\n\n\n\n\nVowel\nWord\nSpeaker\nTokens\n\n\n\n\ni\nbeat\nGretchen\n10\n\n\ni\nbeat\nLauren\n8\n\n\ni\nbelieve\nGretchen\n25\n\n\ni\nbelieve\nLauren\n23\n\n\ni\npeople\nGretchen\n981\n\n\ni\npeople\nLauren\n679\n\n\nu\nblue\nGretchen\n29\n\n\nu\nblue\nLauren\n11\n\n\nu\nthrough\nGretchen\n123\n\n\nu\nthrough\nLauren\n120\n\n\nu\nwho\nGretchen\n512\n\n\nu\nwho\nLauren\n329\n\n\næ\nbang\nGretchen\n4\n\n\næ\nbang\nLauren\n2\n\n\næ\nhand\nGretchen\n48\n\n\næ\nhand\nLauren\n31\n\n\næ\nlaugh\nGretchen\n7\n\n\næ\nlaugh\nLauren\n5\n\n\nɑ\nball\nGretchen\n15\n\n\nɑ\nball\nLauren\n11\n\n\nɑ\nfather\nGretchen\n9\n\n\nɑ\nfather\nLauren\n7\n\n\nɑ\nhonorific\nGretchen\n4\n\n\nɑ\nhonorific\nLauren\n5\n\n\nɔ\nbought\nGretchen\n5\n\n\nɔ\nbought\nLauren\n4\n\n\nɔ\ncore\nGretchen\n7\n\n\nɔ\ncore\nLauren\n2\n\n\nɔ\nwrong\nGretchen\n49\n\n\nɔ\nwrong\nLauren\n21\n\n\nə\namong\nGretchen\n18\n\n\nə\namong\nLauren\n6\n\n\nə\nfamous\nGretchen\n19\n\n\nə\nfamous\nLauren\n18\n\n\nə\nsupport\nGretchen\n42\n\n\nə\nsupport\nLauren\n24\n\n\nɛ\nbet\nGretchen\n13\n\n\nɛ\nbet\nLauren\n9\n\n\nɛ\nguest\nGretchen\n22\n\n\nɛ\nguest\nLauren\n3\n\n\nɛ\nsays\nGretchen\n93\n\n\nɛ\nsays\nLauren\n40\n\n\nɪ\nbit\nGretchen\n267\n\n\nɪ\nbit\nLauren\n242\n\n\nɪ\nfinish\nGretchen\n8\n\n\nɪ\nfinish\nLauren\n11\n\n\nɪ\npin\nGretchen\n18\n\n\nɪ\npin\nLauren\n5\n\n\nʊ\ncould\nGretchen\n444\n\n\nʊ\ncould\nLauren\n174\n\n\nʊ\nfoot\nGretchen\n13\n\n\nʊ\nfoot\nLauren\n6\n\n\nʊ\nput\nGretchen\n250\n\n\nʊ\nput\nLauren\n116\n\n\nʌ\nanother\nGretchen\n239\n\n\nʌ\nanother\nLauren\n130\n\n\nʌ\nbut\nGretchen\n1785\n\n\nʌ\nbut\nLauren\n1049\n\n\nʌ\nfun\nGretchen\n187\n\n\nʌ\nfun\nLauren\n57\n\n\n\n\n\nGretchen and Lauren each say all of the words more than once, and most of the words have 5+ tokens to pick from.\n\n\n\n1.4 Get Timestamps\nThe transcripts on the Lingthusiasm website have all the speakers labelled, but no timestamps, and the captions on YouTube have timestamps, but few speaker labels. To find where in the episodes the token words are, we’ll have to combine them.\nWe’re going to be getting the caption data and audio from Lingthusiasm’s “all episodes” playlist, using the pytube package.\n\nvideo_list = Playlist('https://www.youtube.com/watch?v=xHNgepsuZ8c&' +\n                      'list=PLcqOJ708UoXQ2wSZelLwkkHFwg424u8tG')\n\nThis function uses the YouTube playlist link to download captions for each video:\n\ndef get_captions(videos):\n    \"\"\"Get and format captions from YouTube URLs.\"\"\"\n1    df = pd.DataFrame()\n\n2    for url in videos:\n        video = YouTube(url)\n3        video.bypass_age_gate()\n\n4        title = video.title\n        number = int(title[:2])\n\n5        caps = caption_version(video.captions)\n        try:\n6            caps = parse_captions(caps.xml_captions)\n        except AttributeError:\n            caps = pd.DataFrame()\n\n7        caps.insert(0, 'Episode', number)\n8        df = pd.concat([df, caps])\n\n9    df = df[['Episode', 'Text', 'Start', 'End']]\n    df[['Start', 'End']] = df[['Start', 'End']].astype('float32')\n\n    return df.sort_values(['Episode', 'Start'])\n\n\n1\n\nStart dataframe for results.\n\n2\n\nGo through the list of video URLs and open each one as a YouTube object.\n\n3\n\nNeed to include this to access captions.\n\n4\n\nThe video title is an attribute of the YouTube object, and the episode number is the first word of the title.\n\n5\n\nLoad captions, which returns a dictionary with the language name as the key and the captions and the values. Select which English version to use (function defined below).\n\n6\n\nConvert XML data to dataframe (function defined below). If this doesn’t work, make an empty placeholder dataframe.\n\n7\n\nAdd Episode column (integer).\n\n8\n\nAdd results from current video to dataframe of all results.\n\n9\n\nReturn dataframe of all captions, after organizing columns and sorting rows by episode number then time.\n\n\n\n\nNow, some helper functions to select which caption version to download and then reformat the data from XML to a dataframe.\nCaption data is most commonly formatted as XML or SRT, which is a text format, but not easily convertible to a dataframe.\n\n1def caption_version(cur_captions):\n    \"\"\"Select which version of the captions to use (formatting varies).\"\"\"\n\n    if 'en' in cur_captions.keys():\n        return cur_captions['en']\n    elif 'en-GB' in cur_captions.keys():\n        return cur_captions['en-GB']\n    elif 'a.en' in cur_captions.keys():\n        return cur_captions['a.en']\n\n\ndef caption_times(xml):\n    \"\"\"Find timestamps in XML data.\"\"\"\n\n2    lines = pd.Series(xml.split('&lt;p')[1:], dtype='string')\n3    df = lines.str.extract(r'(?P&lt;Start&gt;t=\"[0-9]*\")')\n4    df['Start'] = df['Start'].str.removeprefix('t=\"').str.removesuffix('\"')\n    df['Start'] = df['Start'].astype('int')\n\n    return df\n\n\ndef caption_text(xml):\n    \"\"\"Find text and filter out extra tags in XML data.\"\"\"\n\n5    lines = pd.Series(xml.split('&lt;p')[1:], dtype='string')\n6    tags = r'\"[0-9]*\"|t=|d=|w=|a=|ac=|&lt;/p&gt;|&lt;/s&gt;|&lt;s\\s|&gt;\\s|&gt;|&lt;/body|&lt;/timedtext'\n7    texts = lines.str.replace(tags, '', regex=True)\n    texts = texts.str.replace('&#39;', '\\'') \\\n        .str.replace('&quot;', \"'\") \\\n        .str.replace('&lt;', '[') \\\n        .str.replace('&gt;', ']') \\\n8        .str.replace('&amp;', ' and ')\n    texts = [clean_text(line) for line in texts.to_list()]\n\n9    return pd.DataFrame({'Text': texts}, dtype='string')\n\n\ndef parse_captions(xml):\n    \"\"\"Combine timestamp rows and text rows.\"\"\"\n\n10    times = caption_times(xml)\n    texts = caption_text(xml)\n    df = times.join(texts)\n11    df = df[df['Text'].str.contains(r'[a-z]')]\n    df = df.reset_index(drop=True)\n12    df['End'] = df['Start'].shift(-1) - 1\n\n    return df\n\n\n1\n\nPrefer en captions, and if those aren’t available, try en-GB then a.en. The main difference is in the formatting, and I think if they are user- or auto-generated.\n\n2\n\nThe XML data is one giant string. Split into a list using the &lt;p tag, which results in one string per caption item (timestamps, text, and various other tags).\n\n3\n\nEach caption item has a start time preceded by t=. This regex gets t=\"[numbers]\" and puts each result into a column called Start.\n\n4\n\nRemove t=\" from the beginning of the Start column and \" from the end, leaving a number that can be converted from a string to an integer.\n\n5\n\nThe XML data is one giant string. Split into a list using the &lt;p tag, which results in one string per caption item (timestamps, text, and various other tags).\n\n6\n\nThis is a list of the other variables that come with the caption text, including t= for start time and d= for duration. The formatting varies somewhat based on which version of captions you get, and the reason this selects the en captions before the en-GB and a.en captions is that this formatting is simpler.\n\n7\n\nTake all the tags/variables out, leaving only the actual caption text.\n\n8\n\nRemove special characters and extra spaces.\n\n9\n\nReturn as dataframe, with Text specified as a string column. If you leave it as the default object type, pandas will throw warnings later.\n\n10\n\nGet dataframes with caption times and caption texts, using two functions just defined. Join the results into one dataframe (this works because both are indexed by row number).\n\n11\n\nRemove rows where caption text is blank, then reset index (row number).\n\n12\n\nMake a column for the End time, which is the Start time of the new row - 1.\n\n\n\n\nRunning this takes a minute or so.\n\n1captions = get_captions(video_list)\n\n2captions = captions[captions['Episode'] &lt;= 84]\n\n3pd.concat([captions.head(), captions.tail()]).style.format(precision=0)\n\n\n1\n\nLoad and format captions for each video in the “all episodes” playlist, using functions defined above.\n\n2\n\nOnly analyze episodes 1-84.\n\n3\n\nShow the first and last 10 rows of the resulting dataframe. .style prints it as a nicer-looking table.\n\n\n\n\n\n\n\n\n\n \nEpisode\nText\nStart\nEnd\n\n\n\n\n0\n1\n[introductory music playing]\n160\n3189\n\n\n1\n1\nL: Welcome to Lingthusiasm a podcast that's\n3190\n6759\n\n\n2\n1\nenthusiastic about linguistics\n6760\n9218\n\n\n3\n1\nI'm Lauren Gawne.\n9219\n11319\n\n\n4\n1\nG: and I'm Gretchen McCulloch and today we're going to be talking about\n11320\n12399\n\n\n449\n84\nyour life who's curious about language. L: Lingthusiasm is created and produced by\n2298000\n2301899\n\n\n450\n84\nGretchen McCulloch and Lauren Gawne. Our Senior Producer is Claire Gawne,\n2301900\n2305559\n\n\n451\n84\nour Editorial Producer is Sarah Dopierala, our Production Assistant is Martha Tsutsui-Billins,\n2305560\n2311679\n\n\n452\n84\nand our Editorial Assistant is Jon Kruk. Our music is 'Ancient City' by The Triangles.\n2311680\n2316359\n\n\n453\n84\nG: Stay lingthusiastic! [Music]\n2316360\nnan\n\n\n\n\n\nSave results to data/captions.csv:\n\ncaptions.to_csv('data/captions.csv', index=False)\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  },
  {
    "objectID": "3_plot_vowels.html",
    "href": "3_plot_vowels.html",
    "title": "Part 3: Plotting the Vowels",
    "section": "",
    "text": "3.1 Setup\n\n1library(tidyverse)\n2library(magrittr)\n3library(ggtext)\n4library(ggforce)\n5library(ggrepel)\n6library(rcartocolor)\n7library(png)\n8library(patchwork)\n\n9options(dplyr.summarise.inform = FALSE)\n\n\n1\n\nData wrangling (tidyr, dplyr, purrr, stringr), ggplot2 for plotting\n\n2\n\nPipe operator\n\n3\n\nMarkdown/HTML formatting for text in plots\n\n4\n\nEllipsis plots\n\n5\n\nOffset text labels from points\n\n6\n\nColor themes\n\n7\n\nOpen PNG images\n\n8\n\nAdd image to plot\n\n9\n\nDon’t print a message every time summarise() is called on a grouped dataframe.\n\n\n\n\n(Note: this could be done in Python, but I strongly prefer the ggplot package for plotting.)\n\n\n\n\n Back to topCitationBibTeX citation:@online{gardner2023,\n  author = {Gardner, Bethany},\n  title = {Lingthusiasm {Vowel} {Plots}},\n  date = {2023-12-13},\n  url = {https://bethanyhgardner.github.io/lingthusiasm-vowel-plots},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGardner, Bethany. 2023. “Lingthusiasm Vowel Plots.”\nDecember 13, 2023. https://bethanyhgardner.github.io/lingthusiasm-vowel-plots."
  }
]