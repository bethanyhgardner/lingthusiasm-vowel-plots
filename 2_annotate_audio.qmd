---
title: "Part 2: Annotating the Audio"
number-offset: [0, 2, 0]
---

<br>

There are two sets of data going into these vowel plots:

1. Vowels pulled from the Lingthusiasm episode recordings, which were located in [Part 1](1_find_words.qmd)
2. Vowels from Gretchen & Lauren recording the Wells lexical set for me

The next steps are to trim the words out of the episode audio files for #1, then annotate the vowels for both #1 and #2.

### Setup

```{python}
#| label: imports

"""Part 2 of Lingthusiasm Vowel Plots: Trimming Audio and Getting Vowel Formants."""

import glob  # <1>
import os  # <1>
import pandas as pd  # <2>
from pytube import Playlist, YouTube  # <3>
from pydub import AudioSegment  # <4>
import parselmouth  # <5>
```

1. File utilities.
2. Dataframes.
3. Getting captions and audio data from YouTube.
4. Working with audio files.
5. Interface with Praat.

Get video info from Lingthusiasm's all episodes [playlist](https://www.youtube.com/watch?v=xHNgepsuZ8c&list=PLcqOJ708UoXQ2wSZelLwkkHFwg424u8tG):

```{python}
#| label: video-list

video_list = Playlist('https://www.youtube.com/watch?v=xHNgepsuZ8c&' +
                      'list=PLcqOJ708UoXQ2wSZelLwkkHFwg424u8tG')
```

Go through each video and download audio (if not already downloaded):

```{python}
#| label: download-audio

def get_audio(videos):
    """Download episode audio from Youtube."""
    for url in videos:
        video = YouTube(url)  # <1>
        video.bypass_age_gate()  # <2>

        title = video.title  # <3>
        episode = int(title[:2])  # <3>

        audio_file_name = os.path.join(  # <4>
            'audio', 'episodes', f'{episode}.mp4')
        if not os.path.isfile(audio_file_name):  # <5>
            audio_stream = video.streams.get_audio_only()  # <5>
            print(f'downloading {episode}')  # <5>
            audio_stream.download(filename=audio_file_name)  # <5>


get_audio(video_list)
```

1. Go through the list of video URLs and open each one as a `YouTube` object.
2. Need to include this to download data.
3. The video title is an attribute of the `YouTube` object, and the episode number is the first word of the title.
4. Create file name for episode audio.
5. If file is not already downloaded, select and download the highest-quality audio-only stream.

### Trim Audio from Episodes

Open the `timestamps` data from Part 1:

```{python}
#| label: open-timestamps

timestamps = pd.read_csv( 
    'data/timestamps_annotate.csv',
    usecols=[
        'Vowel', 'Word', 'Speaker', 'Number',  # <1>
        'Episode', 'Start', 'End'  # <2>
    ],
    dtype={  # <3>
        'Vowel': 'category', 'Word': 'category', 'Speaker': 'category',  # <3>
        'Number': 'category', 'Episode': 'category',  # <3>
        'Start': 'int', 'End': 'int'  # <3>
    }  # <3>
)
timestamps['Speaker'] = timestamps['Speaker'] \
    .str.replace('retchen', '').str.replace('auren', '') \
    .astype('category')  # <4>
```

1. Keep columns specifying word variables.
2. And keep columns specifying where audio is.
3. Make all columns categorical variables, except the `Start` and `End` times (integers).
4. Convert values in `Speaker` column from names to initials.

Trim audio for the duration of the caption (with 250ms before and after). This results ~240 audio files each 2-10sec long, each containing a target word.

```{python}
#| label: trim-audio

def trim_audio(df):
    """Use caption timestamps to trim audio."""

    for i in df.index:  # <1>
        episode = df.loc[i, 'Episode']  # <1>
        word = df.loc[i, 'Word']  # <1>
        speaker = df.loc[i, 'Speaker']  # <1>
        count = df.loc[i, 'Number']  # <1>

        out_file =  os.path.join(  # <2>
            'audio', 'words', f'episode_{word}_{speaker}_{count}.wav')
        if not os.path.isfile(out_file):  # <2>
            in_file = os.path.join('audio', 'episodes', f'{episode}.mp4')  # <3>
            audio = AudioSegment.from_file(in_file, format='mp4')  # <3>
            start = max(df.loc[i, 'Start'] - 250, 0)  # <4>
            end = min(len(audio), df.loc[i, 'End'] + 250)  # <4>
            clip = audio[start:end]  # <4>
            clip.export(out_f=out_file, format='wav')  # <4>


trim_audio(timestamps)
```

1. Go through dataframe that has the example words to annotate and their timestamps.
2. Make file name for current word, and if it does not already exist...
3. Open the audio file for the whole episode.
4. Trim the episode audio to start 250 ms after the caption timestamp and end 250 after the caption timestamp; save it.

### Wells Lexical Set

The Wells lexical set is a set of examples for each vowel/diphthong, chosen to be maximally distinguishable and consistent. You can read more about it on [Wikipedia](https://en.wikipedia.org/wiki/Lexical_set#Standard_lexical_sets_for_English) and [John Wells' blog](https://phonetic-blog.blogspot.com/2010/02/lexical-sets.html). These recordings are going to be more controlled than the vowels pulled from the episode recordings and easier to annotate because they're spoken more slowly and carefully. This set contains some fairly low-frequency words, which is why there's not a lot of overlap with the words pulled from the episodes.

[![The Wells lexical set](resources/wells_lexical_set.jpg)](http://2.bp.blogspot.com/_RSOXNV65lN0/S2a13vcLBAI/AAAAAAAAAYg/RQo2sbM7cqM/s1600-h/sets.jpg)

```{python}
#| label: list-wells-lexical-set

wells_lexical_set = {  # <1>
    '\u0069': 'fleece',   # i
    '\u026A': ['kit', 'near'],  # ɪ
    '\u025B': ['dress', 'square'],  # ɛ
    '\u00E6': ['trap', 'bath'],  # æ
    '\u006F': ['force', 'goat'], # o 
    '\u0075': 'goose',  # u
    '\u028A': ['cure', 'foot'],  # ʊ
    '\u0254': ['cloth', 'north', 'thought'],  # ɔ
    '\u0251': ['lot', 'palm', 'start'],  # ɑ
    '\u028C': 'strut'  # ʌ
}

wells_lexical_set = pd.DataFrame.from_dict(wells_lexical_set, orient='index') \
    .rename(columns={0: 'Word'}) \
    .explode('Word') \
    .reset_index(names='Vowel')  # <2>
```

1. Dictionary where keys are the IPA vowel unicode and values is word(s).
2. Convert to dataframe with columns for `Vowel` and `Word`.

Here's the full set of words for each vowel:

```{python}
#| label: all-words-list

word_list = pd.concat([  # <1>
    pd.DataFrame({  # <2>
        'List': 'lexicalset',  # <2>
        'Vowel': wells_lexical_set['Vowel'],  # <2> 
        'Word': wells_lexical_set['Word']  # <2>
    }),  # <2>
    timestamps[['Vowel', 'Word']].drop_duplicates()  # <3>
  ])
word_list = word_list.fillna('episode')  # <4>
word_list = word_list.sort_values(by = ['Vowel', 'Word'])  # <5>
word_list = word_list.reset_index(drop = True)  # <5>

word_list.style.hide()  # <6>
```

1. Combine word lists from episodes and Wells lexical set.
2. Dataframe for Wells lexical set with columns for `List`, `Vowel`, and `Word`.
3. Subset dataframe for episode word list, with columns for `Vowel` and `List`.
4. Fill the `NA` values of `List` with `episode`.
5. Sort and reset index.
6. Print, not including index.

### An Interlude in Praat

Now, we have about 400 audio clips of Gretchen and Lauren saying words that are good examples of each vowel. The vowel data that will actually be going into the plots is F1 and F2 [**todo: link to overview of formants**]. The easiest way to calculate the vowel formants is using Praat, a software designed for doing phonetic analysis.

Here's an example of what that looked like:

![Praat screenshot of Lauren saying "pit."](resources/praat_screenshot.png)

The vowel [ɪ] is highlighted in pink, and you can see it's darker on the spectrogram than the consonants [k] and [t] before and after it. I placed an annotation (the blue lines) right in the middle of the vowel sound---this one is pretty easy, because Lauren was speaking slowly and without anyone overlapping, so the vowel sound is long and clear.
The formants are the lines of red dots, and the popup window is the formant values at the vowel annotation time. We'll be using F1 (the bottom one) and F2 (the second from the bottom).

You can download Praat and see the documentation [here](https://www.fon.hum.uva.nl/praat/). It's fairly old, so a con is that the interface isn't necessarily intuitive if you're used to modern programs, but a pro is that there are a ton of resources available for learning how to use it. [Here's a tutorial](https://aletheiacui.github.io/tutorials/segmentation_with_praat.html) about getting started with Praat, and [here's one](https://home.cc.umanitoba.ca/~krussll/phonetics/practice/praat.html) for recording your own audio and calculating the formants in it.

After going through all of the audio clips, I had a .TextGrid file (Praat's annotation file format) for each audio clip that includes the timestamp for middle(ish) of the vowel. You can copy formant values manually out of Praat, or you can use Praat scripting to export them to a csv file (see [this  tutorial](https://joeystanley.com/blog/a-tutorial-on-extracting-formants-in-praat/), for example). But I prefer to go back to Python instead of wrangling Praat scripting code.

